% List of key constants:
% c_0 : (d2-color) Determines the number of rounds of initial random tries. Affects initial leeway and sparsity.
% c_1 : (d2-color) The initial leeway (times D). Depends on c_0.
% c_2 : (d2-color) Stopping condition: when to switch to log-leeway routine
% c_3 : (Reduce) # phases 
% c_4 : WAS: (Reduce) Constant factor in being active (calling R-Phase)
% c_5 : WAS (Reduce-Phase) Probability of sending a message along a given path
% c_6 : (Reduce - Analysis) Probability of a query surviving
% c_7 : (Reduce - Analysis) Probability of becoming colored in a phase of Reduce-Phase
% c_{10} : (Similarity) Prob. of entering set S, in similarity graph computation
% c_{11} : (Random H-nbor) Selection of uniformly random d2-neighbor
%% BOunds:
%% c_3 >= 4/c_7 (T:reduce-correct)
%% c_4 >= 8 (L:progress)
%% c_6 = c_4^2/(xxxxx)
%% c_7 = c_6/16 (L:progress)
%% c_{11} \ge max(4,1+log c_3)  (L:rand-nbor)
\section{Randomized Algorithm}
\label{sec:randAlg}
\ym{Let's not forget to include something about the constants later again}
We give randomized {\congest} algorithms that form a \emph{distance-2 coloring} (d2-coloring) using $\Delta^2+1$ colors.

\subsection{Overview of overall algorithm }

Our algorithm has three steps, that are presented and analyzed in the following subsections.
The main component is a method \alg{Reduce} that roughly speaking reduces the density of live nodes in a neighborhood. The case of low-degree graphs is treated separately by the deterministic algorithm of Section \ref{sec:g2-coloring}.

%% 
A node $v$ \emph{trying} a color means that it sends the color to all its immediate neighbors, who then report back if they or any of their neighbors was using (or proposing) that color.
%and have smaller ID than $v$. 
If all answers are negative, then $v$ adopts the color. A node is \emph{live} until it becomes \emph{colored}.

In what follows, $c_0$, $c_1$ and $c_2$ are constants satisfying $c_0 \le 3e/c_1$, $c_1 \le 1/(402 e^3)$ and $c_2 \ge 48$.

\begin{quote}
   \textbf{Algorithm} \emph{d2-Color}
%    \emph{Precondition}: Each node has at most $\tau/\Delta$ immediate live neighbors \\
%    \emph{Postcondition}: The graph is properly $D+1$-colored \\

   0. If $\Delta^2 < c_2 \log n$ then \alg{Deterministic-d2Color($G$)}; halt \\
   1. Form the similarity graphs $H=H_{2/3}$ and $\hat{H} = H_{5/6}$  \hspace{1cm }\textit{// Initial Phase} \\
   2. repeat $c_0 \log n$ times: \\
   \hspace*{2em} Each live node picks a random color and \emph{tries} it. \\
   3. for ($\tau \leftarrow c_1 \Delta^2$; $\tau > c_2 \log n$; $\tau \leftarrow \tau/2$) \hspace{3.5cm }\textit{// Main Phase}\\
\hspace*{2em}      \alg{Reduce}($2 \tau$, $\tau$) \\
    4. \alg{Reduce}($c_2 \log n$, 1)\hspace{6.75cm}\textit{// Final Phase}
%    3. \alg{AccountFor}($\sqrt{d}$)
\end{quote}

 
We show in the next subsection that the first step takes $O(\log n)$ rounds, w.h.p.
The second step clearly takes $\Theta(\log n)$ rounds.
We show that \alg{Reduce}($\phi$,$\tau$) requires time $O((\phi/\tau)^2\log n)$, so \alg{Reduce}($2\tau$,$\tau$) takes $O(\log n)$ rounds, while the last step takes $O(\log^3 n)$ time, w.h.p. Since we also show that at the end every vertex is colored we obtain the following result.

\begin{corollary}
  There is a randomized {\congest} algorithm for d2-coloring with $\Delta^2+1$ colors with time complexity $O(\log^3 n)$, w.h.p.
  \label{C:first-rand-result}
\end{corollary}

We later replace Step 4 by an improved algorithm that reduces the overall time complexity to $O(\log \Delta \log n)$, obtaining a d2-coloring with $\Delta^2+1$-colors.


\subsection{Forming the similarity graphs} 

We form the \emph{similarity graph} $H = H_{2/3}$ on the nodes of $V = V(G)$, where nodes are adjacent only if they are d2-neighbors and have at least $2\Delta^2/3$ d2-neighbors in common.
This is implemented in the sense that each node knows:
a) whether it is a node in $H$, and
b) which of its immediate neighbors are adjacent in $H$.
If a node has no neighbor in $H$, we consider it to be not contained in $H$.

When $\Delta^2 = O(\log n)$, the nodes can learn of all their $H$-neighbors exactly, by simple flooding two hops. In this case, we can define $H$ as edges between d2-neighbors that share at least $2\Delta^3/3$ common d2-neighbors. We focus from here on the case that $\Delta^2 \ge c_{10}\log n$, for appropriate constant $c_{10}$.

To form $H$, each node chooses independently with probability $p = c_{10}(\log n)/\Delta^2$ whether to enter a set $S$. Nodes in $S$ inform their d2-neighbors of that fact. For each node $v$, let $S_v$ be the set of d2-neighbors in $S$. W.h.p., $|S_v| = O(\log n)$ (by Prop.~\ref{P:chernoff}). Each node $v$ informs its immediate neighbors of $S_v$, by pipelining in $O(\log n)$ steps.
Note that a node $w$ can now determine the intersection $S_v \cap S_u$, for its immediate neighbors $v$ and $u$.
Now, d2-neighbors $u$, $v$ are $H$-neighbors iff $|S_v \cap S_u| \ge \myfrac{5}{6} c_{10} \log n$. 
%However, for the purpose of establishing edges in $H$, the node $w$ only considers those intermediate neighbors such that $d(w) \cdot d(v) \ge \Delta^2/4$.
%\ym{Should write the theorem with general $k$ and then have both explicit choices as a corollary. Right now, the second choice is not formal at all. MH: It is a straightforward variation. Not sure if it's worth the effort, given the limited time.}
\begin{theorem}
%\ym{For which $k$ does this work? I am fine with just writing for $k=1/6$ and $k=1/3$. MH: Inserted}
Let $k \in \{3,6\}$.
Let $u,v$ be d2-neighbors. 
If $(u,v) \in H_{1-1/k}$ (i.e., if $|S_v \cap S_u| \ge (1-1/(2k)) c_{10} \log n$), then they share at least $(1-1/k) \Delta^2$ common d2-neighbors, w.h.p.,
while if $(u,v) \not\in H$, then they share fewer than $(1-1/(4k)) \Delta^2$ common d2-neighbors, w.h.p.
\label{T:similarity}
\end{theorem}
%\begin{theorem}
%Let $u,v$ be d2-neighbors. 
%If $(u,v) \in H$ (i.e., if $|S_v \cap S_u| \ge \myfrac{5}{6} c_{10} \log n$), then they %share at least $\nicefrac{2}{3}\, \Delta^2$ common d2-neighbors, w.h.p.,
%while if $(u,v) \not\in H$, then they share fewer than $\nicefrac{11}{12}\, \Delta^2$ common d2-neighbors, w.h.p.
%\label{T:similarity}
%\end{theorem}
The proof is a standard application of Chernoff bounds (\ref{eq:chernoff-upper}-\ref{eq:chernoff-lower}). We give the proof only for $k=3$, for readability.
\begin{proof}
We indicated above how the case $\Delta^2 \le c_{10}\log n$ is treated; we assume from now that $\Delta^2 \ge c_{10}\log n$.
%If $\Delta^2 \le c_{10}\log n$, nodes are $H$-neighbors iff they have at least $4d^2/5$ common d2-neighbors. Assume then that $\Delta^2 \ge c_{10}\log n$.

Let $I_{uv} = G^2[u]\cap G^2[v]$ be the intersection of the d2-neighborhoods of $u$ and $v$. 
For each $w \in I_{uv}$, let $X_w$ be the indicator r.v.\ that $w$ is
selected into the random sample $S$ and let $X = \sum_{w \in I_{uv}} X_w$. Note that $\mu = E[X] = c_{10} (\log n)/\Delta^2 \cdot |I_{uv}|$.

First, suppose $|I_{uv}| \le \nicefrac{2}{3}\, \Delta^2$.
Then $\mu \le \nicefrac{2}{3}\, (c_{10}\log n)$ and
by (\ref{eq:chernoff-upper}), the probability that $u$ and $v$ are neighbors in $H$ is bounded by
  \[ \Pr[uv \in H] = \Pr[X \ge \nicefrac{5}{6}\, c_{10} \log n]
\le \Pr[X \ge \nicefrac{5}{4}\,\mu] \le e^{-\nicefrac{2c_{10}}{3\cdot 48}\, \log n} \le n^{-c_{10}/72}\ . \]
\footnote{Need to check better this last inequality.} 
Thus, setting $c_{10}$ large enough implies that the first half of the claim holds.

Now, suppose $|I_{uv}| \ge \nicefrac{11}{12}\, \Delta^2$.
Then, $\mu \ge \nicefrac{11}{12}\, (c_{10}\log n)$.
By (\ref{eq:chernoff-lower}), the probability that $u$ and $v$ are non-neighbors in $H$ is bounded above by
  \[ \Pr[uv \not\in H] = \Pr[X < \nicefrac{5}{6}\, c_{10} \log n]
\le \Pr[X < \nicefrac{10}{11}\,\mu] \le e^{-\mu/(2\cdot 11^2)} \le e^{-c_{10}/(2\cdot 11 \cdot 12)\, \log n} = n^{-\Omega(c_{10})}\ . \]
Thus, setting $c_{10}$ large enough implies that the second half of the claim holds.
\end{proof}


We also form the graph $\hat{H} = H_{5/6}$ in an equivalent manner.
For $H_{1-k}$, the condition used by the algorithm becomes $|S_v \cap S_u| \ge (1-k/2)c_{10} \log n$ and the case of when $(u,v) \not\in H_{1-k}$ is when they share fewer than $(1-k/4)\Delta^2$ common neighbors.

\subsection{Density and its Properties}

%\paragraph*{Notation and Preliminaries}

The \emph{palette} of available colors is $[\Delta^2] = \{0,1,2,\ldots, \Delta^2\}$. 
%
%Let $G^2[v]$ denote $N_G[N_G[v]]$.
A node is \emph{$\zeta$-sparse} (or \emph{has sparsity} $\zeta$) if $G^2[v]$ contains $\binom{\Delta^2}{2} - \Delta^2\cdot \zeta$ edges.
%\ym{at most that many edges. MH: No, it's better to have it equality.}.\ym{Do all later statements say sparsity "at least"..? MH: They should (usually as sparsity <= xxx)} 
Thus, sparsity is a rational number in the range $0$ to $(\Delta^2-1)/2$.
A node has \emph{slack $q$} if the number of colors of d2-neighbors plus the number of live d2-neighbors is $\Delta^2+1-q$. In other words, a node has slack $q$ if its palette size is an additive $q$ larger than the number of its uncolored $d2$-neighbors. 
%\ym{added this for clarification, correct?} MH: Yes, looks good. Helped correct the exact value.
The \emph{leeway} of a node is its slack plus the number of live d2-neighbors; i.e., it is the number of colors from the palette that are not used among its d2-neighbors.
During our algorithms nodes do not know their leeway and we only use the notion for the analysis.

\paragraph*{Intuition}
% Basic approach for ordinary coloring
A basic approach for randomized distributed algorithms for ordinary $\Delta^2+1$-coloring is for each node to guess a random color that is currently not used among any of its neighbors. Each guess succeeds with constant probability, which leads to logarithmic time complexity. This method doesn't work in the d2-setting because the nodes don't have enough bandwidth to learn the colors of their d2-neighbors.

% Random guesses
Instead, nodes can simply try a random color from the whole palette. If the palette has $(1+\epsilon)\Delta^2$ colors, then this approach succeeds in $O(\log_{1/\epsilon} n)$ rounds. For a $\Delta^2+1$-coloring, we must be more parsimonious. 

% Sparsity
If each neighborhood is sparse, then the first round will result in many neighbors successfully using the same color. 
%By sparse, we mean that the number of triangles involving a given node is at most $(1-\epsilon)\Delta^2$, or \emph{$\epsilon$-sparse}. 
This offers us then the same slack as if we had a larger palette in advance (as proved by \cite{EPS15}), resulting in the same logarithmic time complexity.
%The challenge is then to deal with \emph{dense} neighborhoods. 
%We use the following result of \cite{EPS15}.

\begin{proposition}[\cite{EPS15}, Lemma 3.1]
%If $v$ is a vertex of sparsity $\zeta = \Omega(\log n)$, then the slack of $v$ after the first round of \alg{d2-Color} is at least $\zeta/(4 e^3) = \Theta(\zeta)$, w.h.p.
Let $v$ be a vertex of sparsity $\zeta$ and let $Z$ be the slack of $v$ after the first round of \alg{d2-Color}. Then, 
%\footnote{We changed the constant, because our sparsity definition is slightly different.}
 $\Pr[Z \le \zeta/(4 e^3)] \le e^{-\Omega(\zeta)}$.
\label{P:sparsity}
%\ym{This proposition is so crucial for us that we should provide an explicit citation with a thm number. MH: Added}
\end{proposition}

We let $c_2$ be a constant such that if $\zeta \ge c_2\log n$, then the proposition yields that $Z \ge \sigma/(4e^3)$, w.h.p.
%contrapositive of the above proposition holds with high probability.\ym{Should we explain here what the contrapositive is?} MH: Simplified.

\subsubsection{Structural Properties} 

We first derive the essential features of low-sparsity neighborhoods: almost all d2-neighbors are also $H$-neighbors, and almost all neighbors in $H^2$ are also d2-neighbors.
The first part applies both to $H = H_{2/3}$ and $\hat{H} = H_{5/6}$. We condition on $H$ and $\hat{H}$ being built correctly.

\begin{lemma}
Let $v$ be a node of sparsity $\zeta$.
Then,
\begin{enumerate}
    \item $v$ has at least $\Delta^2 - 8\zeta/k -4/k$ neighbors in $H_{1-k}$, and
    \item The number of nodes that are within distance 2 of $v$ in $H$ but are not d2-neighbors of $v$ is
$|N_{H^2}(v) \setminus N_{G^2(v)}| \le 6\zeta$.
\end{enumerate}
\label{L:h-degree}
\end{lemma}
 
 \begin{proof}
%\label{L:h-degree}
\textbf{1}. 
Let $k' = 1 - k/4$.
A d2-neighbor of $v$ that is not a $H_{1-k}$-neighbor can share at most $k' \Delta^2$ common d2-neighbors with $v$, by Thm.~\ref{T:similarity}, while $H_{1-k}$-neighbors can share up to $\Delta^2-1$ d2-neighbors with $v$.
In other words, the d2-neighbors of $v$ can have degree at most 
$\Delta^2-1$ ($k'\Delta^2$) in $G^2[v]$ if they are $H$-neighbors (non-$H$-neighbors), respectively.
The number of edges in $G^2[v]$ is then at most
\[ \frac{1}{2}\left( |N_{H_{1-k}}(v)| \Delta^2 + (\Delta^2 - |N_{H_{1-k}}(v)|) k' \Delta^2\right) = 
\frac{\Delta^2}{2} \left(k' \Delta^2 + |N_{H_{1-k}}(v)| \frac{k}{4}\right) \ . \]
By the definition of sparsity, the number of edges in $G^2[v]$ equals $\Delta^2((\Delta^2-1)/2 - \zeta)$.
Combining the two bounds, 
\[ |N_{H_{1-k}}(v)| \frac{k}{4} \ge \Delta^2 - 1 - 2\zeta - k'\Delta^2 
  = \Delta^2 \frac{k}{4}  - 1 - 2\zeta \]
Namely, the number of $H_{1-k}$-neighbors of $v$ is bounded below by $\Delta^2 - 8\zeta/k -4/k$.

%\label{L:h2}
\textbf{2}. By sparsity, there are $\nicefrac{1}{2}\, \Delta^2(\Delta^2 - 2\zeta)$ edges within $G^2[v]$. Thus, there are at most $2\zeta \Delta^2$ edges of $H$ that have exactly one endpoint in $N_{G^2}(v)$. Nodes in $N_{H^2}(v)$ share at least $\Delta^2 - \Delta^2/3 - \Delta^2/3 = \Delta^2/3$ d2-neighbors with $v$. Thus, there are at most $6\zeta$ nodes in $H^2[v]$ that are not in $G^2[v]$.
\end{proof}


We frequently work with nodes that are both sparse enough and of small enough leeway. This naturally depends on the current partial coloring at hand.
%\ym{The definition of solid depends on $\phi$. We should point this out.}
\begin{definition}
A node $v$ is said to be \emph{solid} if it has leeway $\phi \le c_1 \Delta^2$ and sparsity $\zeta \le 4 e^3 \phi$. 
\label{D:solid}
\end{definition}

%In particular, the proposition implies the following, by recalling that a $\zeta$-leeway node succeeds in trying a random color with probability $1/\zeta$.

\begin{observation}
Every live node is solid after Step 2 of \alg{d2-Color}, w.h.p.
\label{O:sparse}
\end{observation}

\begin{proof}
Let $v$ be a node of leeway $\phi \ge c_1 \Delta^2$ at the end of Step 2.
Then, in each iteration of the step, the color tried by $v$ has probability $\phi/\Delta^2 \ge c_1$ of being previously unused by d2-neighbors of $v$. Furthermore, the probability that no other d2-neighbor tries the same color in the same round is at least $(1-1/(\Delta^2+1))^{\Delta^2} \ge 1/e$, applying (\ref{eq:inv-e}). Thus,
%These events\ym{These events refers to other nodes trying the same color? MH: No, previously unused vs. tried in the same round} are independent, so 
with probability at least $\phi/(e \Delta^2) \ge c_1/e$, $v$ becomes colored in that round. Hence, the probability that it is not colored in all $c_0 \log n$ rounds is at most $(1-c_1/e)^{c_0\log n} \le e^{-c_0 c_1/e \log n} \le n^{-c_0 c_1/e} \le n^{-3}$, applying the bound $c_0 \ge 3e/c_1$.
So, with probability at least $1-1/n^2$, all nodes have leeway at most $c_1 \Delta^2$ after Step 2.

From the contrapositive of Prop.~\ref{P:sparsity} we obtain that the sparsity of $v$ is at most $\zeta \le 4e^3 \phi$, w.h.p.
\end{proof}

We will in what follows assume that all live nodes are solid, so the statements are conditional on that high-probability event.
%One implication is that after Step 1, each live node has at least $(1-c_1)\Delta^2$ colored neighbors.  \ym{More details}

Let $H'$ denote the subgraph of $\hat{H}[v]$ induced by nodes with a single 2-path to $v$.
Let $deg_H(u)$ denote the number of $H$-neighbors of node $u$.
Solid nodes have many neighbors in $H'$, and its neighbors have many $H$-neighbors.
\begin{lemma}
Let $v$ be a solid node.
Then,
\begin{enumerate} 
  \item $v$ has at least $\Delta^2/2$ $H'$-neighbors.
  \item Every $\hat{H}$-neighbor of $v$ has at least $\Delta^2/3$ $H$-neighbors.
  \item The degree sum in $N_{H'}(v)$ is bounded below by
      \[ \sum_{u \in N_{H'}(v)} deg_H(u) \ge |N_{H'}(v)| (\Delta^2 - c_8\phi), \]
     for some constant $c_8 >0$.
\end{enumerate}
\label{L:H-neighbors}
\end{lemma}

\begin{proof}
\textbf{1}. Let $\zeta$ be the sparsity of $v$ and $\phi$ be its leeway, which satisfy $\phi \le c_1 \Delta^2$ and $\zeta \le 4e^3\phi$, since $v$ is solid.
By Lemma \ref{L:h-degree}(1), $v$ has at least $\Delta^2 - 48\zeta - 24 \ge \Delta^2 - 50\zeta$ neighbors in $\hat{H}$, where $\zeta$ is the sparsity of $v$. At most $\phi$ of those nodes have more than one 2-path to $v$, since $v$'s slack is at most $\phi$. Hence, it has at least $\Delta^2 - 50\zeta - \phi \ge \Delta^2(1 - 201 e^3 c_1) \ge \Delta^2/2$ $\hat{H}$-neighbors with a single 2-path to $v$, using that $c_1 \le 1/(402 e^3)$.

\textbf{2}. 
Let $v$ be a solid node and $u$ a node in $\hat{H}[v]$. Let $X$ be the set of nodes in $G^2[v]$ that share at least $2\Delta^2/3$ d2-neighbors of $G^2[v]$ with $u$, and let $Y$ be the set of d2-neighbors of $u$ in $G^2[v]$.
We want to show that $|X \cap Y| \ge \Delta^2/3$.

Since a node in $\hat{H}[v]$ shares at least $5\Delta^2/6$ d2-neighbors with $v$, any pair of nodes in $\hat{H}[v]$ share at least $\Delta^2 - 2\Delta^2/6 = 2\Delta^2/3$ d2-neighbors in $G^2[v]$. Namely, $|X| \ge |N_{\hat{H}}(v)|$, and by Lemma \ref{L:H-neighbors}(1), $|N_{\hat{H}}(v)| \ge \Delta^2/2$. 
Since $u$ is in $\hat{H}[v]$, $|Y| \ge 5\Delta^2/6$.
Thus, $|X \cap Y| \ge |X| - |N_{G^2}(v)\setminus Y| \ge \Delta^2/2 - (1-5/6)\Delta^2 = \Delta^2/3$.

\textbf{3}. 
Since $v$ is solid, it has sparsity $\zeta \le 4e^3 \phi$. Thus, $G^2[v]$ contains $\binom{\Delta^2}{2} - \zeta \Delta^2$ edges. 
By Lemma \ref{L:h-degree}(1), $v$ has degree $\Delta^2 - 48\zeta - 24$ in $\hat{H}$. The at most 
$48\zeta+24$ nodes in $N_{G^2}(v) \setminus N_{\hat{H}}(v)$ have degree sum at most $(48\zeta+24) \Delta^2$. Thus, the number of edges in $\hat{H}[v]$ is at least  
$\binom{\Delta^2}{2} - (49\zeta +24)\Delta^2 \ge \binom{\Delta^2}{2} - (196e^3\phi+24)\Delta^2$.
%There are at least $\binom{\Delta^2}{2} - O(\phi)\Delta^2$ edges in $\hat{H}[v]$.\footnote{MMH: This remains to be argued.} 
Recall that at most $\phi$ nodes in $N_{\hat{H}}(v)$ can have multiple paths to $v$. 
Then, $H'[v]$ has at most $\phi \Delta^2$ fewer edges than $\hat{H}[v]$, which is still at least $\binom{\Delta^2}{2} - c_8\phi\Delta^2$, for $c_8 = 196e^3+2$. A pair of nodes in $H'[v]$ have at least $2\Delta^2/3$ d2-neighbors of $v$ in common, since they each have at least $5\Delta^2/6$ d2-neighbors in common with $v$. Thus each edge in $H'[v]$ connects $H$-neighbors. In other words, nodes in $H'[v]$ have $\Delta^2 - c_8 \cdot \phi$ $H$-neighbors, on average, for some constant $c_8 > 0$: 
\[ \sum_{u \in N_{H'}(v)} deg_H(u) \ge |N_{H'}(v)| (\Delta^2 - c_8\phi)\ , \]
where $deg_{H}(u)$ denotes the number of $H$-neighbors of node $u$. 
\end{proof}

%%
\iffalse %% MH: It is better to split those into two lemmas, one involving only sparsity and the other solid nodes.
\subsubsection{An attempt to reorganize the structural properties}
\begin{lemma}[Structural Properties]
Assume that xyz holds for the similarity graphs $H$ and $\hat{H}$. ...and assume that all live nodes have leeway $\psi$ ?!?
Then we have
\begin{enumerate}
\item A node of sparsity $\zeta$ has at least $\Delta^2 - 8\zeta/k -4/k$ neighbors in $H_{1-k}$. \ym{Here we do not need that $v$ is live, do we?}
%\label{L:h-degree}
\item Let $v$ be $\zeta$-sparse.
The number of nodes that are within distance 2 of $v$ in $H$ but are not d2-neighbors of $v$ is
$|N_{H^2}(v) \setminus N_{G^2(v)}| \le 6\zeta$.
%\label{L:h2}
\item An $\hat{H}$-neighbor of a solid node has at least $\Delta^2/3$ $H$-neighbors.
%\label{L:H-neighbors}
\item A solid node $v$ has at least $\Delta^2/2$ $\hat{H}[v]$-neighbors with a single 2-path to $v$.
%\label{L:hat-neighbors}
\end{enumerate}
\end{lemma}
\begin{proof}
insert the four proofs. 
\end{proof}
\fi

\subsection{Coloring 'With a Little Help From My Friends'}

\paragraph{Intuition}
We describe here a method to eliminate live nodes of a certain leeway.
The basic idea behind the algorithm is to have already colored nodes "help" the live (i.e., yet uncolored) nodes by checking random colors on their neighborhoods.

% Case of a clique
We can obtain some intuition from the densest case: a $\Delta^2+1$-clique (in $G^2$).
We can recruit the colored nodes to help the live nodes guess a color: if it succeeds for the colored node, it will also succeed for the live node. Each of the $\ell$ live nodes can be allocated approximately $\Delta^2/\ell$ colored node helpers, and in each round, with constant probability, one of them successfully guesses a valid color. Thus, the time complexity becomes $O(\log n)$.
%\ym{More details. For which $\ell$? How does $\ell$ influence this? MH: $\ell$ is the number of live nodes within the clique. Can be any number. Just trying to give basic intuition about how colored nodes can help the live ones.}

% Challenge
The challenge in more general settings is that the nodes no longer have identical (closed) d2-neighborhoods, so a successful guess for one node doesn't immediately translate to a successful color for another node.
To this end, we must deal with two types of errors.
A \emph{false positive} is a color that works for a colored node $w$ but not for its live d2-neighbor $v$, while a \emph{false negative} is a color that fails for the colored node but succeeds for the live node.
It is not hard to conceive of instances where there are no true positives.
\ym{If we have time let's insert a picture. MH: Might be a good idea.}

% Approach
The key to resolving this is to use only advice from nodes that have highly \emph{similar} d2-neighborhoods, or with at least a certain constant fraction of the possible $\Delta^2$ d2-neighbors possible in common. This is captured as a relationship on the nodes: the similarity graph $H$.
To combat false negatives, we also try colors of similar nodes that are not d2-neighbors of the live node but have a common (and similar) d2-neighbor with the live node, i.e., the colors of nodes in $N_{H^2}(v) \setminus N_{G^2}(v)$.
%\ym{The text says non $d2$-neighbors of $v$ the formal statement says non $H$-neighbors of $v$}

% Additional challenges
Additional challenges and pitfalls abound. We must carefully balance the need for progress with the load on each node or edge. Especially, the efforts of the live nodes are a precious resource, %\ym{Is this still true? MH: Yes}, 
but we must allow for their distribution to be decidedly non-random.  
In addition, there are differences between working on 2-paths in $G$ and on edges in $G^2$: there can be multiple 2-paths between d2-neighbors. This can confound seemingly simple tasks such as picking a random d2-neighbor.

\subsubsection{Algorithm \alg{Reduce}}
%\paragraph*{Algorithm} 
After stating the algorithm, we describe in more detail the implementation steps.
We let $c_3$ be a constant to be determined. 
\medskip
%\begin{quote}
% \bigskip

   \textbf{Algorithm} \alg{Reduce}($\phi$, $\tau$) \\
    \emph{Precondition}: Live nodes have leeway less than $\phi$, where $c_2\log n \le \phi \le c_1\Delta^2$ \\
    \emph{Postcondition}: Live nodes have leeway less than $\tau$


%\begin{enumerate}[nosep]
\begin{quote}
   Each node $u$ selects a set $R_u$ of $\rho \doteq c_3 (\phi/\tau)^2 \log n$ random $H$-neighbors \\
    Repeat $\rho$ times \\
    \hspace*{2em} Each live node is \emph{active} with probability $\tau/(8\phi)$ \\
    \hspace*{2em} \alg{Reduce-Phase}($\phi,\tau$) % Each node runs \alg{Reduce-Phase}($\phi,\tau$) independently with prob.\ $\tau/(8 \phi)$
\end{quote}
    
   \textbf{Algorithm} \emph{Reduce-Phase}($\phi$, $\tau$)
\begin{enumerate}
\itemsep 0em 
  \item Each active live node $v$ sends a query across each 2-path to $\hat{H}$-neighbors independently with probability $1/\tau$.
  \item The recipient $u$ of a query $(v,u)$ verifies that there is only a single 2-path from $v$, and otherwise drops the message.
  \item $u$ picks a random color $\hat{c}$ different from its own and checks if it is used by any of its $H$-neighbors. If not, it sends the color back to $v$.
  \item $u$ also forwards the query to the next uniformly random $H$-neighbor $w$ from its list $R_u$, with $w$ appended to the query. % \ym{Whose ID is appended?} MH: I guess it was w
  \item Upon receipt of query $(v,u,w)$, node $w$ checks if $v$ is a d2-neighbor; if not, the color $c(w)$ of $w$ is sent to $v$ (through $u$).
  \item The active live node $v$ tries one proposed $\hat{c}$ and one proposed color $c(w)$, both chosen randomly.
  \end{enumerate}
%\end{quote}
%\bigskip
At each step along the way, a node receiving multiple queries selects one of them at random and drops the others. This can only occur after both rounds of Step 1, first round of Step 2, or second round of Step 4.

\subsubsection{Implementation}
The procedure \alg{Reduce-Phase} takes 23 rounds, or 2 (Step 1), 4 (Step 2), 4 (Step 3), 2 (Step 4), 6 (Step 5), and 5 (Step 6, including the notification of a new color). It is immediate that \alg{Reduce} terminates in $O((\phi/\tau) \log n)$ rounds.



Additional details for specific steps:
\paragraph{Step 1:}
When sending a query along 2-paths in Step 1, the node $v$ simply asks its immediate neighbors to send the queries to all of their immediate neighbors that are $H$-neighbors of $v$, with the given probability.
\paragraph{Step 2:}
Verifying that there is only a single path from $v$ is achieved by asking $u$'s immediate neighbors how many are neighbors of $v$. 

\paragraph{Step 3:}Checking if a color is used by an $H$-neighbor is identical to trying a color, but having the immediate neighbors only taking into account the colors of $u$'s $H$-neighbors.



We detail next how random $H$-neighbors are selected.
Steps 4 to 6 of \alg{Reduce} are straightforward to implement. We note that a query from a live node $v$ maintains a full routing path to $v$, so getting a proposal back to $v$ is simple.

\paragraph{Selecting Random $H$-neighbors}

We detail how the uniform random selection of $H$-neighbors is achieved.
We repeat the following procedure $\rho$ times, to create a list $R_u$ of $\rho$ random $H$-neighbors at each node.

Each node $u$ that receives a query creates a $4\log n$-bit random string $b_u$, and transmits it to all its immediate neighbors. Each node $w$ also picks a $4\log n$-bit random string $r_w$ and sends to immediate neighbors. Now, each immediate node $u'$ computes the bitwise XOR $x_{uw}$ of each string $b_u$ and each string $r_w$ that it receives, where $u$ and $w$ are $H$-neighbors. It forwards $r_w$ to $u$ if and only if the first $2\log \Delta - c_{11} \log\log n$ bits of $x_{uw}$ are zero. 
The node $u$ then selects the $H$-neighbor $w$ with the smallest XORed string $b_u \oplus r_w$. 


\begin{lemma}
The above procedure results in fully independent random $H$-neighbor selection.
\label{L:rand-nbor}
\end{lemma}
\begin{proof}
Each d2-neighbor's string $r_w$ gets forwarded (by $u'$) with probability $2^{c_{11}}\log n / \Delta^2$. Thus, the number  $y_{u}$ of strings that get forwarded to $u$ is expected $E[y_u] = 2^{c_{11}} \log n$. Setting $c_{11} \ge \max(4, 1+\log c_3)$.
By Chernoff (\ref{eq:chernoff-lower}), $u$ receives at least $2^{c_{11}-1} \log n \ge c_3 \log n$ strings with probability $1-1/n^2$.
Thus, with probability at least $1-1/n$, all nodes receive at least $c_3 \log n$ strings.
Hence, the node $u$ will correctly identify the $H$-neighbor whose bitstrings have the smallest XOR with its random string, which gives a uniformly random sampling.
The probability that some pair of nodes receive the same bitstring is at most $2^{-4\log n} = n^{-4}$. Thus, with probability at least $1-1/n^2$, there is always a unique node with the smallest XORed string.

Independence follows because a collection $\{r_w\}_w$ of uniformly random bitstrings that is XORed with a particular (not necessarily random) bitstring $b_u$ forming collection $\{ b_u \oplus r_w\}_w$ stays uniformly random.
The same holds then for the collection of strings $\{b_u\}_u$
that is XORed with a string $r_w$ of a fixed node.
\end{proof}


\subsubsection{Reduce-Phase: Correctness}
During this whole section we assume that all nodes are solid, and that the precondition of \alg{Reduce} is satisfied, i.e., live nodes leeway is at most $\phi$ with $\phi\geq c_2\log n$ for a suitable constant $c_2$. 
We also assume that nothing goes wrong when building the similarity graphs $H$ and $\hat{H}$.
% , that is, \ym{insert details. The statements of \Cref{T:similarity} hold for $H$ for $k=$ and for $\hat{H}$ for $k=$}. % \ym{I do not understand what the assumption of \Cref{P:sparsity} means here. MH: Rewritten}
All statements in this section are conditioned on these events.

The algorithm is based on each live node sending out a host of queries, to random neighbors in $\hat{H}$, and through them to their random $H$-neighbors. We argue that each query has a non-trivial %\ym{i.e, constant? MH: No, about 1/\Delta^2. Even the collection of queries only gives about Omega(tau/\phi)} 
probability of leading to the live node becoming colored.

We say that a given (randomly generated) query \emph{survives} if it is not dropped in any of Steps 1 - 5 due to congestion. It does not account for the outcome of the color tries of Steps 3 and 5.
%\ym{What's with the test in Step 3? Does this proposed color not count as a query? } MH: Good point
%We argue that progress is made in each iteration, either from the random color guesses or from the queries to $H$-neighbors of the $\hat{H}$-neighbors. This is the key argument for the correctness of the algorithm.
%% Each query has good chance of making progress
%\ym{We should clearly write what we mean by a query survives all the steps. It does not mean that it actually comes back with a proposal, does it? I also think that the next lemma talks about a random query originating at a fixed vertex $v$. The survival of queries of different vertices $v$ is not independent. But, I agree that they should not interfere too much. we still need to reason about it} MH: Should be addressed now

\begin{lemma} Let $v$ be a live node. 
Any given query sent from $v$ towards a node $w$ via a node $u\in H'\subseteq \hat{H}[v]$ survives with constant probability at least $c_6 \ge 1/4000$, independent of the path that the query takes.
%its endpoint $w$.
\label{L:survival}
\end{lemma}
\begin{proof}
We fix a particular query $Q$ that has been generated and we use the following notation for nodes on its way (in the case the query does not get dropped) $v-u'-u-w'-w$.
% The query can be dropped only at the following stages: after both rounds of Step 1, first round of Step 2, or second round of Step 4.
%\ym{Let's explicity mention the dropping steps that we refer to here} 
For the intermediate nodes $u'$ and $w'$, queries are only dropped if they are to continue towards the same next destination (i.e., to the same $u$ or $w$). We will deal with them there, e.g.\ by assigning each query a random priority.
\mh{I changed the argument here, to eliminate two steps.}

At the remaining nodes, $u$ and $w$, we bound the expected number of queries arriving -- other than $Q$ -- from above by some constant $c$. By Markov inequality, the number of other queries received is then at most $2c$, with probability at least $1/2$. The probability of surviving the culling with $2c$ other competitors is $1/(2c+1)$. 
Thus, with probability at least $1/(2(2c+1))$, the query $Q$ survives this stage.

%\textit{Being dropped at $u'$:} After the first round, $Q$ lands at an intermediate node $u'$. $u'$ has at most $\phi$ immediate live neighbors, by the algorithm precondition on the leeway of $v$. Of these, expected $\tau/(8\phi)$ are active, and the active ones send a query with probability $1/\tau$.
%Thus, $u'$ receives expected at most $1/8$ other queries. The probability that $Q$ is the only query arriving at $u'$ is at least $7/8$.

\textit{Being dropped at $u$:} After the second round, the node $u$ has at most $(24e^3+1)\phi$ live $H$-neighbors, since $v$ has at most $\phi$ live d2-neighbors and by Lemma \ref{L:h-degree}(2) and Obs.~\ref{O:sparse}, 
there are at most $24e^3\phi$ live nodes that are $H^2$-neighbors of $v$ but not $G^2$-neighbors of $v$. 
Thus, $u$ receives expected at most $(24e^3+1)/8$ other queries.

A query can also be dropped in Step 2 if it arrives at a node in $\hat{H}[v] \setminus H'$, but the lemma is conditioned on queries sent towards nodes in $H'$, i.e., $u\in H'$. 

% \textit{Being dropped at $w'$:} This only occurs if it is sent towards the same $w$, so we deal with it there. \ym{Missing MH: Check}

\textit{Being dropped at $w$:} Finally, we consider a node $w$ at the end of round 2 of Step 4. $w$ has at most $\Delta^2$ $H$-neighbors. Each of its $H$-neighbors with a live $\hat{H}$-neighbor has $H$-degree at least $\Delta^2/3$ by Lemma \ref{L:H-neighbors}(2), and has at most one query to send to a random $H$-neighbor. Hence, the expected number of other queries that $w$ receives is at most 3. 

\textit{Combined probability of being dropped:} The probability of survival is at least 
\[c_6 \ge \frac{1}{2(6e^3 + 5/4)}\cdot \frac{1}{14} \ .  \]
% \frac{7}{8} \cdot \frac{1}{10} \cdot 
\end{proof}

A color is \emph{$v$-good} if it is not used among the d2-neighbors of $v$ at the start of \alg{Reduce-Phase}.

\begin{lemma}
Let $v$ be an active live node at the start of \alg{Reduce-Phase}($\phi,\tau$) and let $\sigma$ be a $v$-good color. The probability that $\sigma$ is proposed to $v$ is at least $c_6/(4\tau)$.
\label{L:sigma}
\end{lemma}
\begin{proof}
We first analyze a hypothetical situation where no queries are dropped.

Suppose $v$ generates a query $Q=(v,u)$ towards a $H'$-neighbor $u$.
%Consider a query $(v,u)$.\ym{Quantify $u$. It is a random query among the queries send?} 
%
We consider two cases, depending on whether the color $\sigma$ appears on an $H$-neighbor of $u$ (at the start of \alg{Reduce-Phase}). We claim that in either case, $\sigma$ gets proposed to $v$ with probability at least $1/\Delta^2$.

\textit{Case 1, $\sigma$ is used by an $H$-neighbor  of $u$: } Let $w$ be an $H$-neighbor of $u$ with color $\sigma$. Then $w$ is not a d2-neighbor of $v$, since $\sigma$ is $v$-good.
%\ym{Just a doublecheck here. This statement is true because we always just run Reduce-Phase for one iteration and then the notion of $v$-good is updated. Remove comment if you agree} MH:Right
With probability at least $1/\Delta^2$, $u$ forwards the query to $w$, who then sends it as proposal to $v$ (given our assumption that the query does not get dropped).

\textit{Case 2, $\sigma$ does not appear among $u$'s $H$-neighbors:} Then with probability $1/\Delta^2$, $u$ will pick $\sigma$ as $\hat{c}$, try it successfully, and propose it to $v$.

Thus, in both cases the probability that a query $Q$ leads to $\sigma$ being proposed to $v$ is at least $1/\Delta^2$, given that $Q$ was generated and that it survives.
The probability that $Q=(v,u)$ is generated is $1/\tau$, and it is independent of it leading to a particular color.
Thus, the probability that a query $Q$ leads to $\sigma$ being proposed to $v$ is at least $1/(\tau\Delta^2)$, given that $Q$ survives. 
%\ym{where does the $\tau$ come from here? MH: Probability of a query being generated. Now 8\phi}

%This event is independent from whether the query is generated along the 2-path $vu$. Thus, with probability $c_6/(\tau\Delta^2)$, a query is sent to $u$ and it leads to $\sigma$ becoming proposed to $v$.

We now consider the event that none of $v$'s queries results in a proposal of $\sigma$. Since no queries are dropped \ym{Why can we assume this for more than one query?}\mh{That is the setting we were considering. The question is more why, later on, we need only consider whether $Q$ survives. I think it's ok, but it should be verified.}, the events for different intermediate nodes $u$ are independent.
Recall that by Lemma \ref{L:H-neighbors}(1), $|N_{H'}(v)| \ge \Delta^2/2$.
Thus, the probability that none of $v$'s queries result in a proposal of $\sigma$ is at most
\[ (1-1/(\tau\Delta^2))^{\Delta^2/2} \le  e^{-1/(2\tau)} \le 1-1/(4\tau)\ , \]
using the inequality $e^{-x} \le 1-x/2$, for $x \le 1/2$.
That is, the probability that there is a query $Q$ in which $\sigma$ is proposed to $v$ is at least $1/(4\tau)$, under our assumption that no queries are dropped.
It only matters that the query $Q$ survives, which happens with probability at least $c_6$, by Lemma \ref{L:survival}, and this is independent of the color it proposes.

Thus, the probability that $\sigma$ gets proposed to $v$ (via some $u \in H'[v]$) is at least 
$c_6/(4\tau)$.
\end{proof}

% Drop-rate for proposals
\begin{lemma}
  The probability that a given color proposal is tried (in Step 6) is at least $c_9 \tau/\phi$, for a constant $c_9$.
\label{L:proposal-tried}
\end{lemma}
\begin{proof}
We first bound the expected number of proposals that $v$ receives.
  Let $P_v$ be the set of nodes that are $H$-neighbors of $H'$-neighbors of $v$ but are not d2-neighbors of $v$. These are the only nodes that generate proposals for $v$ in Step 5. Each $H'$-neighbor $u$ of $v$ receives a query from $v$ with probability $1/\tau$ and sends it to a random $H$-neighbor. $u$ has at least $\Delta^2/3$ $H$-neighbors by Lemma \ref{L:H-neighbors}(2). Thus, the probability that a given node in $P_v$ receives a query involving $v$ is at most $1/\tau \cdot \Delta^2 \cdot 3/\Delta^2 = 3/\tau$. By Lemma \ref{L:h-degree}(2) and Obs.~\ref{O:sparse}, $P_v$ contains at most $24 e^3 \phi$ nodes. Hence, the expected number of proposals $v$ receives from Step 5 is $24 e^3\phi \cdot 3/\tau = 72e^3 \phi/\tau$.

We next bound the expected number of proposals due to Step 3.
The probability $p_u$ that a given $H'$-neighbor $u$ of $v$ picks a color that is not used among its $H$-neighbors, over the random color choices, is 
%\[p_u = (\Delta^2 - deg_{H}(u))/deg_{H}(u) \ge 3(1 - deg_H(u)/\Delta^2) \]
\[p_u = (\Delta^2 - deg_{H}(u))/deg_{H}(u) \ge 3(1 - deg_H(u)/\Delta^2)\ , \] 
where we used Lemma \ref{L:h-degree}(2) in the inequality.
The probability that a random query from $v$ leads to a color proposal is then 
  \[ \frac{\sum_{u \in H'[v]} p_u}{|N_{H'}(v)|} \ge \frac{1}{|N_{H'}(v)|\Delta^2} \sum_{u \in H'[v]} 3 \left(\Delta^2 - deg_H(u)\right) \ge  \frac{c_8 \phi}{|N_{H'}(v)|}\ ,  \]  
applying Lemma \ref{L:h-degree}(3). The expected number of queries sent from $v$ to $H'$-neighbors to $N_{H'}$ is $|N_{H'}(v)|/\tau$, so the expected number of proposals that $v$ receives is $c_8\phi/\tau$.

Let $m$ be a specific proposal and consider the proposals that appear at $v$ at the same time as $m$.
In both cases above, the expected number of proposals received other than $m$ 
\ym{I still don't understand what the proposals refer to and also not the point of this sentence. Before we bounded the proposals that $v$ would get from Step 3 or from Step 5. How can $v$ get any other proposals at all? }\mh{Think of it this way. On average over time, one person is waiting at the coffee machine. But when you go there, there are usually two: you and one more person. Because once you condition on you being there, that changes the equation. How should I word this?}
is at most $c = \max(c_8, 72 e^3)\phi/\tau$. By Markov's inequality, $v$ receives at most $2c$ other proposals, with probability at least $1/2$. Hence, with probability at least $1/(2(2c+1))$ $Q$ will be the proposal selected to be tried.
\end{proof}

\begin{lemma}
An active live node with leeway at least $\tau$ at the start of \alg{Reduce-Phase}($\phi,\tau$) becomes colored with probability $c_7 \tau/\phi$, where $c_7 \ge c_6c_9/32$.
\label{L:progress}
\end{lemma}
\begin{proof}
Let $v$ denote the active live node and let $\Psi$ denote the set of $v$-good colors. By the leeway bound, $|\Psi| \ge \tau$. 
Let $L$ be the set of live d2-neighbors of $v$ that are active in the current phase; these are the nodes whose attempted tries could conflict with $v$'s tries. 
The expected size of $L$ is $E[|L|] \le \tau/(8\phi) \cdot \phi = \tau/8$, since $v$ has at most $\phi$ live d2-neighbors and each is active with probability $\tau/(8\phi)$.
 Let $A$ be the event that $L$ is of size at most $\tau/4$, and note that by Markov's inequality, $\Pr[A] \ge (1-1/2) = 1/2$.
For each $z \in L$ and each $\sigma \in \Psi$, let $p_{z,\sigma}$ be the probability that $z$ tries $\sigma$.
Let $\Psi' = \{\sigma \in \Psi : \sum_{z \in L} p_{z,\sigma} \ge 1/2\}$. 
Note that $\sum_{z \in L} \sum_{\sigma \in \Psi} p_{z,\sigma} \le |L| \le \tau/4$, assuming $A$ holds. On the other hand, the sum is at least $\sum_{\sigma \in \Psi'} \sum_{z \in L} p_{z,\sigma} \ge \sum_{\sigma \in \Psi'} 1/2 = |\Psi'|/2$. Thus, $|\Psi'|/2 \le \tau/4$, or $|\Psi'| \le \tau/2$, assuming $A$.

Let $\hat{\Psi} = \Psi \setminus \Psi'$.
Then, assuming $A$ holds, $|\hat{\Psi}| \ge \tau/2$.
Each color in $\hat{\Psi}$ gets proposed to $v$ with probability at least $c_6/(4\tau)$, by Lemma \ref{L:sigma}, and that proposal becomes a try with probability at least $c_9 \tau/\phi$ \ym{Is this taking care of the dependencies because of this "other proposals paragraph before", that is, if $v$ gets many proposals only one survives? If that's the case we should add this to the algorithm description, if not dependencies between the colors are still here}.\mh{I'm not quite following.}  Let $B$ be the event that $v$ tries a color in $\hat{\Psi}$. So, $B$ holds, assuming $A$, with probability at least 
$\sum_{\sigma \in \hat{\Psi}} c_6/(4\tau) \cdot c_9\tau/\phi = \tau/2 \cdot c_6c_9/(4\phi) = c_6c_9/8 \cdot \tau/\phi$.
%\ym{Insert calculation MH: Attempted}.

%Consider now the possibility of conflict.
Let $C$ denote the event that no other d2-neighbor of $v$ tries the same color as $v$. For each $\sigma \in \hat{\Psi}$, the probability that some live d2-neighbor of $v$ tries $\sigma$ is at most $1/2$. Thus, if $v$ tries a color in $\hat{\Psi}$ and $A$ holds, then no neighbor selects the same color with probability, i.e.\ $\Pr[C|A \cap B] \ge 1/2$.\ym{I agree with the reasoning but something in the formality is awkward. $C$ here reasons about the specific color that is the outcome of $B$. Minor issue} \mh{There is a concern that $C$ may depend on the color $\sigma$. One way, that seems safe, is to consider not only the actual tries that nodes in $L$ make, but all the proposals they might receive if there were no packet drops. That list would be independent of $v$'s color.}

If $A$, $B$, and $C$ all hold, then $v$ will be colored. 
%If $v$ tries a color in $\hat{\Psi}$ and no d2-neighbor of $v$ tries the same color, then $v$ becomes colored.
The probability of this happening is at least 
%all of $A$, $B$ and $C$ hold, or $c_7 = 9c_6/64$.
\[ \Pr[A \cap B \cap C] = \Pr[A] \cdot \Pr[B | A] \cdot \Pr[C | A \cap B] \ge \frac{1}{2}\cdot \frac{c_6c_9 \tau}{8\phi} \cdot \frac{1}{2} = \frac{c_6c_9\tau}{32\phi} \]
%Hence, if $A$ holds and $v$ tries a color in $\hat{\Psi}$, it is successful with probability at least $3/4$.
\end{proof}

The correctness of the algorithm now follows from Lemma \ref{L:progress} by concentration.

\begin{theorem}
%There is a choice of $c_2>1$ such that if all messages of one execution of \alg{Reduce} are delivered the postcondition holds, w.h.p. Namely, all nodes of leeway at least $\tau$ are colored in the call to \alg{Reduce}($\phi,\tau$), w.h.p.
All nodes of leeway at least $\tau$ are colored in the call to \alg{Reduce}($\phi,\tau$), w.h.p.
\label{T:reduce-correct}
\end{theorem}

\begin{proof}
Set $c_3 = 32/c_7$. 
Let $v$ be a live node of leeway at least $\tau$ at the start of $\alg{Reduce}$.
With probability $\tau/(8\phi)$, $v$ is active in a given phase, and with
probability at least $c_7 \tau/\phi$, $v$ becomes colored in a given phase where it is active, by Lemma \ref{L:progress}. Thus, the probability that it remains live after all $\rho = c_3 (\phi/\tau)^2 \log n$ phases is at most 
$(1-c_7\cdot(\tau/\phi)^2/8)^{c_3 (\phi/\tau)^2 \log n} \le e^{-4\log n} \le n^{-4}$.
The probability that some such node remains uncolored is at most $n^{-3}$.
\end{proof}

\iffalse %%%% This does not seem needed anymore. At least, it should be simplified
\begin{proof}
Let $v$ be a node of leeway at least $\tau$ that is live at the start of \alg{Reduce}($\phi,\tau$).
Let $N'_v$ be the set of nodes in $\hat{H}[v]$ that have a single 2-path to $v$, and recall that by Lemma \ref{L:H-neighbors}(1), $|N'_v| \ge \Delta^2/2$.
The number of queries sent from $v$ and not dropped is expected $|N'(v)|/\tau \cdot \log n \ge (\Delta^2 \log n)/(2\tau)$, and by Chernoff at least $(\Delta^2\log n)/(4\tau)$, with probability at least $1-n^2$, since $\tau \le \Delta^2/16$.  
Each succeeds independently with probability at least $\tau/(4\Delta^2)$, by Lemma \ref{L:progress}. 
The probability that none of them succeed is at most
\[ (1 - \tau/(8\Delta^2))^{\Delta^2 \log n)/(2\tau)} \le e^{-1/16 \cdot \log n} \le n^{-1/16} \ . \]
The probability that some node remains live after \alg{Reduce}($\phi,\tau$) is at most $n^{-1/16+1}$.
Setting $c_5 \ge 48$ ensures that this occurs with probability at most $1/n^2$.
\end{proof}
\fi %%%% This does not seem needed anymore. At least, it should be simplified



Observe that after \alg{Reduce}($\phi$,1), all nodes are colored, w.h.p., since a live node always has leeway at least 1. 
Corollary \ref{C:first-rand-result} follows.

% \clearpage

%\section{Handling logarithmic sparsity}
\subsection{Algorithm with Improved Final Phase}

We now give an improved algorithm for d2-coloring that uses $\Delta^2+1$ colors and runs in $O(\log \Delta\log n)$ rounds. We assume $\Delta$ is known to the nodes.
This is achieved by replacing the final phase of \alg{d2-color} (i.e., the last step) with a different approach.
%We now given an improved algorithm for the last step of \alg{d2-Color} that produces a $\Delta^2+1$-coloring runs in $O(\log n)$ rounds. Thus, we let $D=\Delta^2$ here, and assume $\Delta$ is known to the nodes. This improves the overall time complexity of the algorithm to $O(\log \Delta \log n)$.

\paragraph*{Intuition}
In the final phase of the improved algorithm, the nodes cooperate to track the colors used by the d2-neighbors of each live node. Thus, they learning the \emph{remaining palette}: the set of colors of $[\Delta^2]$ not used by d2-neighbors. Gathering the information about a single live node is too much for a single node to accumulate, given the bandwidth limitation. Instead, each live node chooses a set of \emph{handlers}, each handling a subrange of its color spectrum. The colored nodes then need to forward their color to the appropriate handler of each live d2-neighbor. After learning about the colors used, each of the multiple handlers choose an unused color at random and forward it to the live node. The live node selects among the proposed colors at random and tries it (which works with constant probability).

Since no routing information is directly available, we need to be careful how the coloring information is gathered at the handlers. We use here a \emph{meet-in-the-middle} approach. Each handler informs a random subset of its d2-neighbors about itself and each colored node sends out its message along a host of short random walks. In most cases, if the numbers are chosen correctly, a random walk will find an informed node, which gets the message to the handler. 

Once the unused palette is available, the coloring can be finished up in $O(\log n)$ rounds in the same fashion as the basic randomized {\congest} algorithm for ordinary coloring.

\begin{quote}
   \textbf{Algorithm} \emph{Improved-d2-Color}
%    \emph{Precondition}: Each node has at most $\tau/\Delta$ immediate live neighbors \\
%    \emph{Postcondition}: The graph is properly $D+1$-colored \\

   If $\Delta^2 \ge c_2\log n$ then  \\
%   1. Each node picks a random color and keeps it if none of its d2-neighbors picked it. \\
   \hspace*{2em} repeat $c_0 \log n$ times: \\
%   1. repeat $\Theta(\log n)$ times: \\
   \hspace*{4em} Each live node picks a random color and \emph{tries} it. \\     
   \hspace*{2em} Form the similarity graphs $H=H_{2/3}$ and $\hat{H} = H_{5/6}$ \\
   \hspace*{2em} for ($\tau \leftarrow c_1 \Delta^2$; $\tau > c_2\log n$; $\tau \leftarrow \tau/2$) \\
   \hspace*{4em}      \alg{Reduce}($2 \tau$, $\tau$) \\
    \alg{LearnPalette}() \\
    \alg{FinishColoring()}
\end{quote}

The main effort of this section is showing how to learn the remaining palette in $O(\log n)$ steps.
%\section{Improved Final Phase}
We first show how that information makes it easy to color the remaining nodes.

\paragraph{Finishing the coloring}

Suppose each node has $O(\log n)$ live d2-neighbors and knows the remaining palette. This includes the case when $\Delta^2 \le c_2\log n$, in which case no d2-neighbors are yet colored. 
We can then simulate the basic randomized algorithm for ordinary colorings with constant overhead, to complete the coloring in $O(\log n)$ rounds.

This algorithm, \alg{FinishColoring}, proceeds as follows:
Each node $v$ repeats the following two-round procedure until it is successfully colored. 
Flipping a random coin, $v$ is quiet or tries a random color from $T'_v$, with equal probability $1/2$.
If it succeeds, it forwards that information to immediate neighbors. They promptly forward it to each of their immediate neighbors $w$, who promptly updated their remaining palette $T'_w$. If a node has a backlog of color notifications to forward, it sends out a \textsc{Busy} message. A node with a \textsc{Busy} neighbor then waits (stays quiet) until all notifications have been forwarded (and all \textsc{Busy} signals have been lifted from its immediate neighbors). 
  %  \item $v$ tries a random color from $T'_v$ until it is successfully colored. When it becomes colored, it informs the other live d2-neighbors of its color, who update their set $T'_v$ accordingly.
  %the colors in $T'_v$ in sequence, until colored.

\begin{lemma}
\alg{FinishColoring} completes in $O(\log n)$ rounds, w.h.p.
\label{l:step7}
\end{lemma}

\begin{proof}
A node waits for a busy neighbor for at most $O(\log n)$ rounds, since it has that many live d2-neighbors.
Consider then a non-waiting round.
With probability 1/2, at least half of the live d2-neighbors of $v$ are quiet. In this case, at least half of the colors of $v$'s palette are not tried by d2-neighbors, and hence, $v$ succeeds with probability at least $1/2$. The expected number of rounds is therefore $O(\log n)$, and by Chernoff (\ref{eq:concentration}), this holds also w.h.p.
\end{proof}

\paragraph{Learning the Available Palette}
%
Let $\varphi \le c\log n$ be an upper-bound on the leeway of live nodes.
Let $Z$ and $P$ be quantities to be determined.
We call each set $B_i = \{i\cdot \Delta^2/Z, i\cdot \Delta^2/Z+1, i\cdot \Delta^2/Z+2, \ldots, (i+1)\Delta^2/Z-1 \}$, $i=0,1,\ldots, Z-1$, a \emph{block} of colors. The last block $B_{Z-1}$ additionally contains the last color, $\Delta^2$. 
There are then $Z$ blocks % $B_1,\ldots,B_Z$ 
that partition the whole color space $[\Delta^2]$.

\bigskip

%\begin{quote}
   \textbf{Algorithm} \emph{LearnPalette}() \\
    \emph{Precondition}: Live nodes have leeway at most $\varphi \le c_2\log n$. \\
%    , and each node has at most $\varphi/\Delta$ live immediate neighbors. \\
  \emph{Postcondition}: Live nodes know their remaining palette
%  \emph{Postcondition}: The graph is properly $d2$-colored.

\begin{enumerate}
  \item If $\Delta = O(\log n)$, then the nodes learn the remaining palette in $\Delta$ rounds by flooding, and halt.
  \item Each node learns of its live d2-neighbors by flooding.
  \item For each live node $v$ and each block $i \in \{1,\ldots, \Delta\}$ of colors, a random $H$-neighbor $z^i_v$ of $v$ is chosen.
  % We verify that $v$ and $z^i_v$ share at least 1/2 of their d2-neighbors, by comparing $O(\log n)$-sized random samples of their neighborhoods, reselecting $z^i_v$ if needed.
  \item Each node $z^i_v$ picks a random subset $Z_v^i$ of $P$ d2-neighbors (formed as a set of random 2-hop paths).\footnote{Do we need to constrain these nodes to be d2-neighbors of $v$?}
  It informs them that it "handles" block $i$ of the palette of live node $v$ (which indirectly tells them also the 2-path back to $z^i_v$). 
  \item Each colored node $u$ with color $c_u$ attempts to forward its color to some node in $Z_v^i$, where $i = \lfloor c(u) / \Delta \rfloor$, for each live d2-neighbor $v$. 
  This is done by sending the color along $\Theta(\Delta^2/P \cdot \log n)$ different random 2-paths. The node in $Z_v^i$ then forwards it directly to $z^i_v$. Let $C^i_v$ denote the set of colors that $z^i_v$ learns of.
  \item Each node $z_v^i$ informs $v$ by pipelining of the set $T_v^i = B_i \setminus C_v^i$ of colors missing within its range. 
  \item $v$ informs its immediate neighbors by pipelining of $T_v= \cup_i T_v^i$, the colors that it has not learned of being in its neighborhood. Each such node $w$ returns the set $\hat{T}_{v,w}$, consisting of the colors in $T_v$ used among $w$'s immediate neighbors. $v$ removes those colors from $T_v$ to produce $T'_v = T_v \setminus \cup_w \hat{T}_{v,w}$, which yields the true remaining palette $[\Delta^2] \setminus T'_{v}$.
%  \item $v$ informs all its d2-neighbors of $T_v= \cup_i T_v^i$ (by pipelining).
%  \item Each d2-neighbor of $v$ whose color appears in $T_v$ informs $v$ of its color. Those colors are removed from $T_v$ to produce $T'_v \subseteq T_v$.
\end{enumerate}
%\end{quote}
%  Observe that $T'_v$ maintains the current set of available colors for $v$, after Step 6 and through each iteration of Step 7. 
 \medskip
 
We first detail how a node $u$ selects a set of $m$ random d2-neighbors, as done in Steps 3, 4, and 5. It picks $m$ edges (with replacement) to its immediate neighbors at random and informs each node $w$ of the number $m_w$ of paths it is involved in. Each immediate neighbor $w$ then picks $m_w$ immediate neighbors. This way, $u$ does not directly learn the identity of the d2-neighbors it selects, but knows how to forward messages to each of them. Broadcasting or converge-casting individual messages then takes time $\max_{w \in N_G(v)} m_w$, which is $O(m/\Delta + \log n)$, w.h.p. (by (\ref{eq:concentration})).

The key property of this phase is the following.

\begin{lemma}
$|T_v| = O(\log n)$, for every live node $v$, w.h.p.
%If a live node $v$ is not $\zeta$-sparse, then all but $O(\zeta)$ colors of $v$'s d2-neighbors get recorded in $C_v = \cup_i C^i_v$, w.h.p.
\label{l:last}
\end{lemma}

\begin{proof}
By assumption, live node $v$ has leeway $O(\log n)$ at the start of the algorithm, and thus it has slack $O(\log n)$. 
By the contrapositive of Prop.~\ref{P:sparsity}, it is $\zeta$-sparse, for $\zeta=O(\log n)$. 
Thus, the $H$-degree of $v$ is at least $\Delta^2 - 40\zeta$, by Lemma~\ref{L:h-degree}(1). For all $H$-neighbors of $v$, a random 2-hop walk has probability at least $|Z_v^i|/\Delta^2 = P/\Delta^2$ of landing in $Z_v^i$. 
Thus, w.h.p., one of the $\Theta((\Delta^2/P)\log n)$ random walks ends there, resulting in the color being recorded in $C_v$. 
Hence, w.h.p., $|T_v| \le |N_{G^2}(v)\setminus N_H(v)| \le 40\zeta = O(\log n)$.
\end{proof}

% The correctness is immediate from the statement of the algorithm. 
We now argue the round complexity of the algorithm.

\begin{theorem}
%The time complexity of \alg{AccountFor}($\varphi$) is $O(\varphi^{2/3}/\Delta^{1/3} \cdot \log n)$.
The time complexity of \alg{LearnPalette}($\varphi$) with $\varphi=O(\log n)$ is $O(\log n)$, when $\Delta = \Omega(\log n)$.
\label{T:learnpalette}
\end{theorem}
%
\begin{proof}
Each live node has $\Omega(\Delta^2)$ neighbors and selects $Z$ of them uniformly to become handling nodes. Thus, each node has probability $O(Z/\Delta^2)$ of becoming a handling node for a given live d2-neighbor, and since it has $O(\varphi)$ live d2-neighbors (by the precondition), it becomes a handling node for an expected $O(\varphi \cdot Z/\Delta^2)$ live nodes. 
%\footnote{Here we use a weaker form of the precondition}
\begin{itemize}
\item The flooding in Step 2 takes as many rounds as there are live nodes in a immediate neighborhood, which is $O(\varphi)$.
\item In Step 3 involves sending a single message to each handling node (of each live node), which is easily done in $O(1)$ expected time, or $O(\log n)$ time, w.h.p.
\item In Step 4, each node forwards expected $P/\Delta$ messages from each handling immediate neighbor, and it has $O(Z\varphi/\Delta)$ such immediate neighbors. 
Thus, it sends out $O(P Z\varphi/\Delta^2)$ messages to random neighbors, or $O(P Z \varphi/\Delta^3)$ message per outgoing edge, w.h.p. This takes time $O(PZ \varphi/\Delta^3)$. 
\item In Step 5, a colored node needs to forward its color to the handling node $z_v^i$ of each of its $O(\varphi)$ live d2-neighbors. Since it is sent along $\Delta^2/P \log n$ paths, and due to the conductance,  w.h.p., the color reaches a node in $Z_v^i$ (the set of nodes informed of $z_v^i$). 

The path from a colored node $u$ to a handler $z_v^i$ for a live node has two parts: the path $p_{u,w}$ from $u$ to an node $w$ that knows the path to $z_v^i$, and path $q_{w,z_v^i}$ from $w$ to $z_v^i$. 
A given node $a$ has probability $1/\Delta^2$ of being an endpoint of a given path $p_{u,w}$ (from a d2-neighbor); there are $O(\varphi)$ live d2-neighbors (by the precondition, weaker form), $\Delta^2$ colored d2-neighbors, and $\Delta^2/P \log n$ copies of messages sent about each. Thus, a given node has expected 
\[ O(\varphi \cdot \Delta^2 \cdot \Delta^2/P \log n \cdot 1/\Delta^2) = O(\varphi/P \cdot \Delta^2 \log n) \] 
random paths going to it.
Similar argument holds for a node being an intermediate node on a path $p_{u,w}$: the number of immediate neighbors goes to $\Delta$, while the probability of being a middle point on the path goes to $1/\Delta$, resulting in the same bound.
Thus, the load on each edge is $O(\Delta \varphi/P \log n)$. 

For the $q$-paths, the main congestion is going into the handler. Observe that there are only $O(\log n)$ $p$-paths that reach an informed node $w$. Hence, the number of paths going into a given handler is the product of the size of the block, times $\log n$: $\Delta^2/Z \cdot \log n)$. So, the load on an incoming edge into a handler is $O(\Delta/Z \log n)$, w.h.p. (when $Z = O(\Delta)$).

\item The pipelining of Steps 6-7 takes $O(|T_v|)$ rounds, and by Lemma \ref{l:last}, $|T_v| = O(\log n)$, w.h.p. 

\end{itemize}

To summarize, the dominant terms of the time complexity are $O(\log n)$ (Steps 2, 6-7), $O(PZ\varphi/\Delta^3) = O(PZ (\log n)/\Delta^3)$ (Step 4), $O(\Delta\varphi/P \log n) = O(\Delta(\log n)^2/P)$ (first half of Step 5), and $O(\Delta/Z \log n)$ (second half of Step 5).

Optimizing, we set $Z = \Delta$ and $P = \Delta \sqrt{\Delta\log n}$, 
for time complexity of $O(\log n (1 + \sqrt{(\log n)/\Delta}))$, which is $O(\log n)$ when $\Delta = \Omega(\log n)$. 
\end{proof}




Combining Thm.~\ref{T:learnpalette} and Lemma \ref{l:step7} with the results of the previous subsection, we obtain the following bound on \alg{Improved-d2-Color}.

\begin{theorem}
\label{thm:d2ColoringRand}
\alg{Improved-d2-Color} properly d2-colors a graph with $\Delta^2+1$ colors in $O(\log \Delta \log n)$ rounds, w.h.p.
\end{theorem}
