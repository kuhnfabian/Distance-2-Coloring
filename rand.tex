% List of key constants:
% c_0 : (d2-color) Determines the number of rounds of initial random tries. Affects initial leeway and sparsity.
% c_1 : (d2-color) The initial leeway (times D). Depends on c_0.
% c_2 : (d2-color) Stopping condition: when to switch to log-leeway routine
% c_3 : (Reduce) # phases 
% c_6 : (Reduce - Analysis) Probability of a query surviving
% c_7 : (Reduce - Analysis) Probability of becoming colored in a phase of Reduce-Phase
% c_8 : (Used in structure) Degree sum related
% c_9 : (Reduce-Phase) Prob proposal is tried
% c_{10} : (Similarity) Prob. of entering set S, in similarity graph computation
% c_{11} : (Random H-nbor) Selection of uniformly random d2-neighbor
%% BOunds:
%% c_3 >= 4/c_7 (T:reduce-correct)
%% c_{11} \ge max(4,1+log c_3)  (L:rand-nbor)

\section{Randomized Algorithm}
\label{sec:randAlg}
%%\ym{Let's not forget to include something about the constants later again} MH: TO DO
We give randomized {\congest} algorithms that form a d2-coloring using $\Delta^2+1$ colors. We use the prominent space at the beginning of the section to introduce notation that we use frequently throughout the proofs in this section.

%\noindent\textbf{Notation.}
\paragraph{Notation}
The \emph{palette} of available colors is $[\Delta^2] = \{0,1,2,\ldots, \Delta^2\}$. 
The neighbors in $G$ of a node are called \emph{immediate neighbors}, while the neighbors in $G^2$ are \emph{d2-neighbors}.
For a (sub)graph $K$, let $N_K(v)$ denote the set of neighbors of $v$ in $K$, and let $K[v] = K[N_K(v)]$ denote the subgraph induced by these neighbors.
A node is \emph{live} until it becomes \emph{colored}.

A node has \emph{slack $q$} if the number of colors of d2-neighbors plus the number of live d2-neighbors is $\Delta^2+1-q$. In other words, a node has slack $q$ if its palette size is an additive $q$ larger than the number of its uncolored $d2$-neighbors. 
The \emph{leeway} of a node is its slack plus the number of live d2-neighbors; i.e., it is the number of colors from the palette that are not used among its d2-neighbors.
During our algorithms nodes do not know their leeway and we only use the notion for the analysis.

When we state that an event holds \emph{w.h.p.} (with high probability), we mean that for any $c > 0$, we can choose the constants involved so that the event holds with probability $1 -O(n^{-c})$. 

%% Intuition
\subsection{Overview : Coloring 'With a Little Help From My Friends'}
% \subsection{Overview of Algorithm }

% \paragraph*{Intuition}
% Basic approach for ordinary coloring
As explained in the introduction, the simple approach for coloring $G$ -- for each node to guess a random color that is currently not used among any of its neighbors -- fails for d2-coloring because the nodes do not have enough bandwidth to learn the colors of their d2-neighbors. 

% Random guesses
Instead, nodes can certainly \emph{try} a random color from the whole palette. The node's immediate neighbors can maintain their immediate neighbors colors, and thus can answer if a certain color conflicts with the current coloring (or other colors being tried).
This works well in the beginning, until most of the node's neighbors are colored.
If the palette has $(1+\epsilon)\Delta^2$ colors, then this approach alone succeeds in $O(\log_{1/\epsilon} n)$ rounds, but for a $\Delta^2+1$-coloring, we must be more parsimonious. 
%\ym{Should we say that iterating the approahc takes $O(\Delta\log n)$ rounds} MH: It's actually $\Delta^2\log n$, but I don't think it's worth it.
% Sparsity
If each neighborhood is sparse, then the first round will result in many neighbors successfully using the same color. 
This offers us then the same slack as if we had a larger palette in advance, as proved formally by Elkin, Pettie and Su \cite{EPS15}, resulting in the same logarithmic time complexity.
The challenge is then to deal with dense neighborhoods, of varying degrees of sparsity, defined formally for each node as the average non-degree of the subgraph induced by its neighborhood in $G^2$.
We tackle this with the algorithm \alg{Reduce}, that successfully colors all nodes in a given range of color slack (and by extension, sparsity range).
%\ym{reference the formal definition of sparsity? I do not get the part in the brackets}


%\mypar{Reduce}
The basic idea behind the \alg{Reduce} algorithm is to have the colored nodes "help" the \emph{live} (i.e., yet uncolored) nodes by checking random colors on their neighborhoods.
%
% Case of a clique
We can obtain some intuition from the densest case: a $\Delta^2+1$-clique (in $G^2$).
We can recruit the colored nodes to help the live nodes guess a color: if it succeeds for the colored node, it will also succeed for the live node. Each of the $\ell$ live nodes can be allocated approximately $\Delta^2/\ell$ colored node helpers, and in each round, with constant probability, one of them successfully guesses a valid color. This reduces the number of live nodes by a constant factor, leading to a $O(\log n)$ time complexity.

% Challenge
The challenge in more general settings is that the nodes no longer have identical (closed) d2-neighborhoods, so a successful guess for one node does not immediately translate to a successful color for another node. To this end, we must deal with two types of errors.
A \emph{false positive} is a color that works for a colored node $w$ but not for its live d2-neighbor $v$, while a \emph{false negative} is a color that fails for the colored node but succeeds for the live node. It is not hard to conceive of instances where there are no true positives.
%\ym{If we have time let's insert a picture. MH: Might be a good idea.} We probably won't have the space, after all...

% Approach
The key to resolving this is to use only advice from nodes that have highly \emph{similar} d2-neighborhoods.
%, or with at least a certain constant fraction of the possible $\Delta^2$ d2-neighbors possible in common.\ym{I don't get the part starting with 'or with at least a', but I also think that it is not needed and should be erased} MH: Ok, erased
This is captured as a relationship on the nodes: the similarity graph $H\subseteq G^2$. We also use another similarity graph $\hat{H}\subseteq G^2$, with a higher threshold for similarity (in terms of number of common d2-neighbors).
To combat false negatives, we also try colors of similar nodes that are not d2-neighbors of the live node but have a common (and similar) d2-neighbor with the live node, i.e., we try the colors of nodes in $N_{H^2}(v) \setminus N_{G^2}(v)$.

% Additional challenges
Additional challenges and pitfalls abound. We must carefully balance the need for progress with the load constraints on each node or edge. Especially, the efforts of the live nodes are a precious resource, but we must allow for their distribution to be decidedly non-random.  
In addition, there are differences between working on 2-paths in $G$ and on edges in $G^2$: there can be multiple 2-paths between d2-neighbors. This can confound seemingly simple tasks such as picking a random d2-neighbor.

%\mypar{Improved Algorithm}
Once bounds on sparsity and slack drop below logarithmic, concentration results fail to hold. 
%This includes the case of low-degree graphs.
Finishing up becomes the bottleneck of the whole algorithm.
For this, we introduce an improved algorithm. The key is that there is now sufficient bandwidth for the remaining live nodes to learn the \emph{complement} of the set of colors of their d2-neighbors: the colors that they \emph{don't use}. Though there is no obvious way for them to discover that alone, they can again get help from the colored node in tallying the colors used. This becomes a different problem of outsourcing and load-balancing, but one that is aided by the extreme denseness of the parts of the graph that are not yet fully colored. We explain this in more detail in Sec.~\ref{ssec:improved}.
Once the palette is known, the rest of the algorithm is like for the basic randomized algorithm for coloring $G$, since the nodes can maintain an up-to-date view of the colors of their d2-neighbors.


\subsection{Algorithm Description}

We now outline our top-level algorithm, followed by the main routine, \alg{Reduce} and details on the implementation. %The analysis is given in later subsections.
%Our algorithm has three steps, that are presented and analyzed in the following subsections.


%% 
Recall that a node $v$ \emph{trying} a color means that it sends the color to all its immediate neighbors, who then report back if they or any of their neighbors was using (or proposing) that color.
%and have smaller ID than $v$. 
If all answers are negative, then $v$ adopts the color. 

In what follows, $c_0$, $c_1$ are constants satisfying $c_0 \le 3e/c_1$, $c_1 \le 1/(402 e^3)$. Also, $c_2$ is a sufficiently large constant needed for concentration.

\begin{quote}
   \textbf{Algorithm} \alg{d2-Color}
%    \emph{Precondition}: Each node has at most $\tau/\Delta$ immediate live neighbors \\
%    \emph{Postcondition}: The graph is properly $D+1$-colored \\

   0. If $\Delta^2 < c_2 \log n$ then \alg{Deterministic-d2Color($G$)}; halt \\
   1. Form the similarity graphs $H=H_{2/3}$ and $\hat{H} = H_{5/6}$  \hspace{1cm }\textit{// Initial Phase} \\
   2. repeat $c_0 \log n$ times: \\
   \hspace*{2em} Each live node picks a random color and \emph{tries} it. \\
   3. for ($\tau \leftarrow c_1 \Delta^2$; $\tau > c_2 \log n$; $\tau \leftarrow \tau/2$) \hspace{3cm }\textit{// Main Phase}\\
\hspace*{2em}      \alg{Reduce}($2 \tau$, $\tau$) \\
    4. \alg{Reduce}($c_2 \log n$, 1)\hspace{5.75cm}\textit{// Final Phase}
%    3. \alg{AccountFor}($\sqrt{d}$)
\end{quote}

For low-degree graphs, we use in Step 0 the deterministic algorithm from Sec.~\ref{ssec:sumDeltaDeta}. The similarity graphs $H$ and $\hat{H}$ that are constructed in Step 1 are  used later  (in \alg{Reduce}) to decide which nodes assists whom. The point of Step 2 is to reduce the initial number of live nodes down to a small fraction of each neighborhood. We can then apply the main algorithm, \alg{Reduce}, to progressively reduce the leeway of live nodes (by coloring them or their neighbors).

%\subsubsection{Algorithm \alg{Reduce}}
%After stating the algorithm, we describe in more detail the implementation steps.
We let $c_3$ be a sufficiently large constant to be determined. 
\medskip

   \textbf{Algorithm} \alg{Reduce}($\phi$, $\tau$)
   
    \emph{Precondition}: Live nodes have leeway less than $\phi$, where $c_2\log n \le \phi \le c_1\Delta^2$
    
    \emph{Postcondition}: Live nodes have leeway less than $\tau$

\begin{quote}
   Each node $u$ selects a multiset $R_u$ of $\rho \doteq c_3 (\phi/\tau)^2 \log n$ random $H$-neighbors (with replacement) \\
    Repeat $\rho$ times: \\
    \hspace*{2em} Each live node is \emph{active} independently with probability $\tau/(8\phi)$ \\
    \hspace*{2em} \alg{Reduce-Phase}($\phi,\tau$) 
\end{quote}

%\ym{add forward pointer to the selection of $R_u$}
The selection of random $H$-neighbors needs care and is treated in the following subsection.
\alg{Reduce}($\phi,\tau$) ensures that all nodes with a certain range of leeway get colored, which implicitly ensures that the number of live nodes in each neighborhood goes down as well. To avoid too much competition between live nodes, only a fraction of them participate in any given phase.
%% MH: TO DO
%\ym{The whole text around the pseudocode could use a bit of polishing in the sense that it doesn't have a super nice flow. Some parts are not touched or discussed (e.g. $R_u$, the precondition and postcondition should appear a bit earlier in the text earlier. }
%\ym{Maybe we have an outline that describes the pseudocode. Except for technicalities the algorithm consists of log n random color trials and log (Delta) phases of reduce. One phase of reduce reduces the leeway until... then we have the final phase. I think that would be very helpful}
    
   \textbf{Algorithm} \alg{Reduce-Phase}($\phi$, $\tau$)
\begin{enumerate}
\itemsep 0em 
  \item Each active live node $v$ sends a query across each 2-path to $\hat{H}$-neighbors independently with probability $1/(6000\phi)$.
  \item The recipient $u$ of a query $(v,u)$ verifies that there is only a single 2-path from $v$, and otherwise drops the message.
  \item $u$ picks a random color $\hat{c}$ different from its own and checks if it is used by any of its $H$-neighbors. If not, it sends the color back to $v$ as a proposal.
  \item $u$ also forwards the query to the next uniformly random $H$-neighbor $w$ from its list $R_u$, with $w$ appended to the query.
  \item Upon receipt of query $(v,u,w)$, node $w$ checks if $v$ is a d2-neighbor; if not, the color $c(w)$ of $w$ is sent to $v$ (through $u$).
  \item The active live node $v$ tries a color chosen uniformly random among the proposed colors (if any).
  \end{enumerate}
At each step along the way, a node receiving multiple queries selects one of them at random and drops the others. This can only occur after both rounds of Step 1, first round of Step 2, or second round of Step 4.
%\ym{drops are different for step 1. no drop at intermediate node here as pointed out later}

\alg{Reduce-Phase} ensures that all active live nodes (with leeway between $\tau$ and $\phi$) get colored with a "constant" probability (i.e., a constant times $\tau/\phi$). This is achieved by each live node recruiting a large subset of its similar d2-neighbors to try random colors (in Step 3). This is a probabilistic filter that reduces the workload of the live nodes. These neighbors also check the colors of their neighbors (in Step 5) to see if those might be suitable for the live node. The key idea is that one of these forms of assistance is likely to be successful, and that it is possible to share the load effectively.

\mypar{Implementation}
%
Additional details for specific steps of \alg{Reduce-Phase}:

\emph{Step 1:}
When sending a query along 2-paths in Step 1, the node $v$ simply asks its immediate neighbors to send the queries to all of their immediate neighbors that are $H$-neighbors of $v$, with the given probability.
%\ym{i.e., no drop at the intermediate node} MH: No, only the accounting

\emph{Step 2:}
Verifying that there is only a single path from $v$ is achieved by asking $u$'s immediate neighbors how many are neighbors of $v$. 

\emph{Step 3:} Checking if a color is used by an $H$-neighbor is identical to trying a color, but having the immediate neighbors only taking into account the colors of $u$'s $H$-neighbors.

We detail in the following subsection how $R_u$, the collection of random $H$-neighbors, is generated in Step 4 in time proportional to its size. 
%are selected in Step 4.\ym{need to mention $R_u$. how long does building $R_u$ take? $O(log n)$? We now want neighbors with replacement in $R_u$?} 
Steps 5 and 6 of \alg{Reduce-Phase} are straightforward to implement. We note that a query from a live node $v$ maintains a full routing path to $v$, so getting a proposal back to $v$ is simple.


\mypar{Complexity}
%
For low-degree graphs ($\Delta^2 = O(\log n)$), we use the deterministic algorithm of \Cref{thm:d2ColoringDelta}, which runs in $O(\Delta^2 + \log^* n) = O(\log n)$ rounds.
We show in the next subsection that the first step of \alg{d2-Color} takes $O(\log n)$ rounds, w.h.p.
The second step clearly takes $\Theta(\log n)$ rounds.

%\ym{What about building $R_u$}
The procedure \alg{Reduce-Phase} takes 23 rounds, or 2 (Step 1), 4 (Step 2), 4 (Step 3), 2 (Step 4), 6 (Step 5), and 5 (Step 6, including the notification of a new color).
%Building $R_u$ takes $O((\phi/\tau)\log n)$ rounds. 
Thus, the round complexity of \alg{Reduce} (including the time to generate $R_u$) is proportional to the number of iterations of the loop, or $O((\phi/\tau)^2 \log n)$.
It follows that all the steps of \alg{d2-Color} run in $O(\log n)$ time, except the last step, i.e., $\alg{Reduce}(c_2\log n,1)$, that requires $O(\log^3 n)$ time.
Since we also show that at the end every vertex is colored, w.h.p., we obtain the following result.

\begin{corollary}
  There is a randomized {\congest} algorithm to d2-color with $\Delta^2+1$ color in $O(\log^3 n)$ rounds, w.h.p.
  \label{C:first-rand-result}
\end{corollary}


%\input{prelim.tex}  %% Chernoff Bounds moved to appendix

Outline of the rest of this section: In Sec.~\ref{ssec:similarity} we describe the remaining supporting steps of the algorithms, in Sec.~\ref{ssec:dense} we derive key structural properties of non-sparse graphs, and in Sec.~\ref{ssec:correctness} we prove the correctness of the algorithms. 
%
Then, in Sec.~\ref{ssec:improved}, we present an improved algorithm that replaces the last step of \alg{d2Color} to reduce the overall time complexity to $O(\log \Delta \log n)$, giving our main result, Thm.~\ref{thm:d2ColoringRand}.

\subsection{Support Functions : Similarity Graphs and Random Neighbor Selection}
\label{ssec:similarity}
We describe here in more details the support tools and property used in our algorithm.
This includes the formation of the similarity graph, and the selection of random d2-neighbors. 

\mypar{Forming the similarity graphs} 
%\subsection{Forming the similarity graphs} 
%
%
We form the \emph{similarity graph} $H = H_{2/3}$ on the nodes of $V = V(G)$, where nodes are adjacent only if they are d2-neighbors and have at least $2\Delta^2/3$ d2-neighbors in common.
This is implemented in the sense that each node knows:
a) whether it is a node in $H$, and
b) which of its immediate neighbors are adjacent in $H$.
If a node has no neighbor in $H$, we consider it to be not contained in $H$.

When $\Delta^2 = O(\log n)$, the nodes can learn of all their $H$-neighbors exactly, by simple flooding two hops. In this case, we can define $H$ as edges between d2-neighbors that share at least $2\Delta^2/3$ common d2-neighbors. We focus from here on the case that $\Delta^2 \ge c_{10}\log n$, for appropriate constant $c_{10}$.

To form $H$, each node chooses independently with probability $p = c_{10}(\log n)/\Delta^2$ whether to enter a set $S$. Nodes in $S$ inform their d2-neighbors of that fact. For each node $v$, let $S_v$ be the set of d2-neighbors in $S$. W.h.p., $|S_v| = O(\log n)$ (by Prop.~\ref{P:chernoff}). Each node $v$ informs its immediate neighbors of $S_v$, by pipelining in $O(\log n)$ steps.
Note that a node $w$ can now determine the intersection $S_v \cap S_u$, for its immediate neighbors $v$ and $u$.
Now, d2-neighbors $u$, $v$ are $H$-neighbors iff $|S_v \cap S_u| \ge \myfrac{5}{6} c_{10} \log n$. 
%
\begin{theorem}
Let $k \in \{3,6\}$.
Let $u,v$ be d2-neighbors. 
If $(u,v) \in H_{1-1/k}$ (i.e., if $|S_v \cap S_u| \ge (1-1/(2k)) c_{10} \log n$), then they share at least $(1-1/k) \Delta^2$ common d2-neighbors, w.h.p.,
while if $(u,v) \not\in H$, then they share fewer than $(1-1/(4k)) \Delta^2$ common d2-neighbors, w.h.p.
\label{T:similarity}
\end{theorem}
%\begin{theorem}
%Let $u,v$ be d2-neighbors. 
%If $(u,v) \in H$ (i.e., if $|S_v \cap S_u| \ge \myfrac{5}{6} c_{10} \log n$), then they %share at least $\nicefrac{2}{3}\, \Delta^2$ common d2-neighbors, w.h.p.,
%while if $(u,v) \not\in H$, then they share fewer than $\nicefrac{11}{12}\, \Delta^2$ common d2-neighbors, w.h.p.
%\label{T:similarity}
%\end{theorem}
The proof uses Chernoff bounds and is deferred to the appendix. 
%\ym{The link to the equations is not helpful}
% MH: TO DO
%\ym{I think we can save a lot of space in the first half of this subsection}

We also form the graph $\hat{H} = H_{5/6}$ in an equivalent manner.
For $H_{1-k}$, the condition used by the algorithm becomes $|S_v \cap S_u| \ge (1-k/2)c_{10} \log n$ and the case of when $(u,v) \not\in H_{1-k}$ is when they share fewer than $(1-k/4)\Delta^2$ common neighbors.

\mypar{Selecting random $H$-neighbors}
%
We detail how the multiset $R_u$ of uniformly random $H$-neighbors is form, at the start of \alg{Reduce}.
%\ym{Explain where, or call it throughout $R_u$ creation. We now change from random $H$-neighbor selection, $R_u$, in step 3, before the reduce and so on.}
We repeat the following procedure $\rho$ times, to create a list $R_u$ of $\rho$ random $H$-neighbors at each node:
Each node $u$ that receives a query creates a $4\log n$-bit random string $b_u$, and transmits it to all its immediate neighbors. Each node $w$ also picks a $4\log n$-bit random string $r_w$ and sends to immediate neighbors. Now, each immediate node $u'$ computes the bitwise XOR $x_{uw}$ of each string $b_u$ and each string $r_w$ that it receives, where $u$ and $w$ are $H$-neighbors. It forwards $r_w$ to $u$ if and only if the first $2\log \Delta - c_{11} \log\log n$ bits of $x_{uw}$ are zero. 
The node $u$ then selects the $H$-neighbor $w$ with the smallest XORed string $b_u \oplus r_w$. 


\begin{lemma}
A multiset $R_u$ of independent uniformly random $H$-neighbors of node $u$ can be generated in $O(|R_u|+\log n)$ rounds. 
%The above procedure results in fully independent random $H$-neighbor selection.
\label{L:rand-nbor}
\end{lemma}
\iffalse %%% Moved to appendix
\begin{proof}
Each d2-neighbor's string $r_w$ gets forwarded (by $u'$) with probability $2^{c_{11}}\log n / \Delta^2$. Thus, the number  $y_{u}$ of strings that get forwarded to $u$ is expected $E[y_u] = 2^{c_{11}} \log n$. Setting $c_{11} \ge \max(4, 1+\log c_3)$.
By Chernoff (\ref{eq:chernoff-lower}), $u$ receives at least $2^{c_{11}-1} \log n \ge c_3 \log n$ strings with probability $1-1/n^2$.
Thus, with probability at least $1-1/n$, all nodes receive at least $c_3 \log n$ strings.
Hence, the node $u$ will correctly identify the $H$-neighbor whose bitstrings have the smallest XOR with its random string, which gives a uniformly random sampling.
The probability that some pair of nodes receive the same bitstring is at most $2^{-4\log n} = n^{-4}$. Thus, with probability at least $1-1/n^2$, there is always a unique node with the smallest XORed string.

Independence follows because a collection $\{r_w\}_w$ of uniformly random bitstrings that is XORed with a particular (not necessarily random) bitstring $b_u$ forming collection $\{ b_u \oplus r_w\}_w$ stays uniformly random.
The same holds then for the collection of strings $\{b_u\}_u$
that is XORed with a string $r_w$ of a fixed node.
\end{proof}
\fi %%% moved to appendix

%\mypar{Properties of Dense Subgraphs}
\subsection{Properties of Dense Subgraphs}
\label{ssec:dense}

The example of the clique at the start of this subsection shows that dense subgraphs have the advantage that the views of the nodes are homogeneous. The advantage of sparse subgraphs is that they will invariable have slack, as shown by the following result of \cite{EPS15}.

We frequently work with nodes that are both sparse enough and of small enough leeway. 

\begin{definition}
A node $v$ is \emph{$\zeta$-sparse} (or \emph{has sparsity} $\zeta$) if $G^2[v]$ contains $\binom{\Delta^2}{2} - \Delta^2\cdot \zeta$ edges. 
$v$ is \emph{solid} if it has leeway $\phi \le c_1 \Delta^2$ and sparsity $\zeta \le 4 e^3 \phi$. 
\label{D:solid}
\end{definition}

Sparsity is a rational number in the range $0$ to $(\Delta^2-1)/2$ that is fixed throughout. Leeway is a decreasing property of the current partial coloring. Thus, once a node becomes solid, it stays solid throughout the algorithm.
%\ym{I would put this into a definition environment. It is super central to the paper and can easily be missed} MH: Right.
Elkin, Pettie and Su \cite{EPS15} formalized the connection between the two properties.

% We use the following result of \cite{EPS15}.

\begin{proposition}[\cite{EPS15}, Lemma 3.1]
%If $v$ is a vertex of sparsity $\zeta = \Omega(\log n)$, then the slack of $v$ after the first round of \alg{d2-Color} is at least $\zeta/(4 e^3) = \Theta(\zeta)$, w.h.p.
Let $v$ be a vertex of sparsity $\zeta$ and let $Z$ be the slack of $v$ after the first round of \alg{d2-Color}. Then, 
%\footnote{We changed the constant, because our sparsity definition is slightly different.}
 $\Pr[Z \le \zeta/(4 e^3)] \le e^{-\Omega(\zeta)}$.
\label{P:sparsity}
\end{proposition}

We require the constant $c_2$ to be such that if $\zeta \ge c_2\log n$, then the contraposition of Prop.~\ref{P:sparsity} yields that $Z \ge \zeta/(4e^3)$, w.h.p. 
%\ym{The $\sigma$ here should be a $\zeta$?} MH: Yes, thanks!

%\subsubsection{Structural Properties} 
%\label{ssec:struct}
We derive some of the essential features of low-sparsity neighborhoods: almost all d2-neighbors are also $H$-neighbors, and almost all neighbors in $H^2$ are also d2-neighbors.
The first part applies both to $H = H_{2/3}$ and $\hat{H} = H_{5/6}$. 
%We condition on $H$ and $\hat{H}$ being built correctly.\ym{we should say what this means, appears later again} MH: Do that later.

\begin{lemma}
Let $v$ be a node of sparsity $\zeta$.
Then,
\begin{enumerate}
    \item $v$ has at least $\Delta^2 - 8\zeta/k -4/k$ neighbors in $H_{1-k}$, and
    \item The number of nodes that are within distance 2 of $v$ in $H$ but are not d2-neighbors of $v$ is
$|N_{H^2}(v) \setminus N_{G^2(v)}| \le 6\zeta$.
\end{enumerate}
\label{L:h-degree}
\end{lemma}
 

%In particular, the proposition implies the following, by recalling that a $\zeta$-leeway node succeeds in trying a random color with probability $1/\zeta$.
%The proof of the following observation is deferred to \Cref{app:struct}.
\begin{observation}
Every live node is solid after Step 1 of \alg{d2-Color}, w.h.p.
\label{O:sparse}
\end{observation}


Let $H'$ denote the subgraph of $\hat{H}[v]$ induced by nodes with a single 2-path to $v$.
Let $deg_H(u)$ denote the number of $H$-neighbors of node $u$.
Solid nodes have many neighbors in $H'$, and its neighbors have many $H$-neighbors.
\begin{lemma}
Let $v$ be a solid node.
Then,
\begin{enumerate} 
  \item $v$ has at least $\Delta^2/2$ $H'$-neighbors.
  \item Every $\hat{H}$-neighbor of $v$ has at least $\Delta^2/3$ $H$-neighbors.
  \item The degree sum in $N_{H'}(v)$ is bounded below by
      \[ \sum_{u \in N_{H'}(v)} deg_H(u) \ge |N_{H'}(v)| (\Delta^2 - c_8\phi), \]
     for constant $c_8 \le 4000$.
\end{enumerate}
\label{L:H-neighbors}
\end{lemma}

%% \subsection{Coloring 'With a Little Help From My Friends'}

\subsection{Correctness of Reduce}
\label{ssec:correctness}
%
Recall that the leeway of a node counts the number of colors of the palette that are not used among its neighbors. It counts both the number of uncolored nodes and the color slack that follows from the node being solid (by Obs.~\ref{O:sparse}). 
%\ym{Don't know where, but we should somewhere say how leeway and the number of uncolored neighbors relate. Somewhere it says naturally bounds. But we should discuss this a bit more prominently}
During this whole section we assume that all nodes are solid, and that the precondition of \alg{Reduce} is satisfied, i.e., live node's leeway is at most $\phi$ with $\phi\geq c_2\log n$ for a large enough constant $c_2$.
We also assume that similarity graphs $H$ and $\hat{H}$ are correctly constructed, in the sense of Thm.~\ref{T:similarity}. %\ym{Say what we mean by this}
All statements in this section are conditioned on these events.
% \mh{remember: State this early}

The algorithm is based on each live node sending out a host of queries, to random neighbors in $\hat{H}$, and through them to random $H$-neighbors. We argue that each query has a non-trivial probability of leading to the live node becoming colored.
We say that a given (randomly generated) query \emph{survives} if it is not dropped in any of Steps 1 - 5 due to congestion. This does not account for the outcome of the color tries of Steps 3 and 5.

Missing proofs are given in Sec.~\ref{app:improved}

\begin{lemma} Let $v$ be an active live node. 
Any given query sent from $v$ towards a node $w$ via a node $u\in H'\subseteq \hat{H}[v]$ survives with constant probability at least $c_6 \ge 1/7$, independent of the path that the query takes.
\label{L:survival}
\end{lemma}
\iffalse %%% Proof omitted in conference version
\begin{proof}
We fix a particular query $Q$ that has been generated and we use the following notation for nodes on its way (in the case the query does not get dropped) $v-u'-u-w'-w$.
% The query can be dropped only at the following stages: after both rounds of Step 1, first round of Step 2, or second round of Step 4.
For the intermediate nodes $u'$ and $w'$, queries are only dropped if they are to continue towards the same next destination (i.e., to the same $u$ or $w$). We will deal with them there, e.g.\ by assigning each query a random priority.

At the remaining nodes, $u$ and $w$, we bound the expected number of queries arriving -- other than $Q$ -- from above by some constant $c$. By Markov inequality, the number of other queries received is then at most $2c$, with probability at least $1/2$. The probability of surviving the culling with $2c$ other competitors is $1/(2c+1)$. 
Thus, with probability at least $1/(2(2c+1))$, the query $Q$ survives this stage.

%\textit{Being dropped at $u'$:} After the first round, $Q$ lands at an intermediate node $u'$. $u'$ has at most $\phi$ immediate live neighbors, by the algorithm precondition on the leeway of $v$. Of these, expected $\tau/(8\phi)$ are active, and the active ones send a query with probability $1/\tau$.
%Thus, $u'$ receives expected at most $1/8$ other queries. The probability that $Q$ is the only query arriving at $u'$ is at least $7/8$.

\textit{Being dropped at $u$:} After the second round, the node $u$ has at most $(24e^3+1)\phi$ live $H$-neighbors, since $v$ has at most $\phi$ live d2-neighbors and by Lemma \ref{L:h-degree}(2) and Obs.~\ref{O:sparse}, 
there are at most $24e^3\phi$ live nodes that are $H^2$-neighbors of $v$ but not $G^2$-neighbors of $v$. $u$ receives a query from each with probability $1/(6000\phi)$, for an expected at most $(24e^3+1)\phi/(6000\phi) \le 1/8$ queries.

A query can also be dropped in Step 2 if it arrives at a node in $\hat{H}[v] \setminus H'$, but the lemma is conditioned on queries sent towards nodes in $H'$, i.e., $u\in H'$. 

% \textit{Being dropped at $w'$:} This only occurs if it is sent towards the same $w$, so we deal with it there. \ym{Missing MH: Check}

\textit{Being dropped at $w$:} Finally, we consider a node $w$ at the end of round 2 of Step 4. $w$ has at most $\Delta^2$ $H$-neighbors. Each of its $H$-neighbors with a live $\hat{H}$-neighbor has $H$-degree at least $\Delta^2/3$ by Lemma \ref{L:H-neighbors}(2), 
and has expected at most $1/8$ query to send to a random $H$-neighbor. 
%and has at most one query to send to a random $H$-neighbor. 
Hence, the expected number of other queries that $w$ receives is at most $3/8$. 

%% Note: Values changed
\textit{Combined probability of being dropped:} The probability of survival is at least $c_6 \ge 1/(2(1/4+1)) 1/(2(3/4+1)) \cdot = 4/35 \ge 1/9$.
%\ge 1/22 \cdot 1/14 = 1/308$.  
%\[c_6 \ge \frac{1}{22}\cdot \frac{1}{14} \ .  \]
% \frac{7}{8} \cdot \frac{1}{10} \cdot 
\end{proof}
\fi %% proof omitted

The following progress lemma is the core of our correctness argument.
A color is \emph{$v$-good} if it is not used among the d2-neighbors of $v$ at the start of \alg{Reduce-Phase}.

\begin{lemma}
Let $v$ be an active live node at the start of \alg{Reduce-Phase}($\phi,\tau$) and let $\sigma$ be a $v$-good color. The probability that $\sigma$ is proposed to $v$ is at least $c_6/(24000\phi)$.
\label{L:sigma}
\end{lemma}
\begin{proof}
We first analyze a hypothetical situation where no queries are dropped.

Suppose $v$ generates a query $Q=(v,u)$ towards a $H'$-neighbor $u$.
%Consider a query $(v,u)$.\ym{Quantify $u$. It is a random query among the queries send?} 
%
We consider two cases, depending on whether the color $\sigma$ appears on an $H$-neighbor of $u$ (at the start of \alg{Reduce-Phase}). We claim that in either case, $\sigma$ gets proposed to $v$ with probability at least $1/\Delta^2$.

\textit{Case 1, $\sigma$ is used by an $H$-neighbor  of $u$: } Let $w$ be an $H$-neighbor of $u$ with color $\sigma$. Then $w$ is not a d2-neighbor of $v$, since $\sigma$ is $v$-good.
%\ym{Just a doublecheck here. This statement is true because we always just run Reduce-Phase for one iteration and then the notion of $v$-good is updated. Remove comment if you agree} MH:Right
With probability at least $1/\Delta^2$, $u$ forwards the query to $w$, who then sends it as proposal to $v$.

\textit{Case 2, $\sigma$ does not appear among $u$'s $H$-neighbors:} Then with probability $1/\Delta^2$, $u$ will pick $\sigma$ as $\hat{c}$, try it successfully, and propose it to $v$.

Thus, in both cases the probability that a query $Q$ leads to $\sigma$ being proposed to $v$ is at least $1/\Delta^2$, given that $Q$ was generated and that it survives.
The probability that $Q=(v,u)$ is generated is $1/(6000\phi)$, and it is independent of it leading to a particular color.
Thus, the probability that a query $Q$ leads to $\sigma$ being proposed to $v$ is at least $1/(6000\phi\Delta^2)$, given that $Q$ survives. 
%\ym{where does the $\tau$ come from here? MH: Probability of a query being generated. Now 8\phi}

%This event is independent from whether the query is generated along the 2-path $vu$. Thus, with probability $c_6/(\tau\Delta^2)$, a query is sent to $u$ and it leads to $\sigma$ becoming proposed to $v$.

We now consider the event that none of $v$'s queries results in a proposal of $\sigma$. Since we are in the setting where no queries are dropped, 
%\ym{Why can we assume this for more than one query?}\mh{That is the setting we were considering. The question is more why, later on, we need only consider whether $Q$ survives. I think it's ok, but it should be verified.}, 
the events for different intermediate nodes $u$ are independent.
Recall that by Lemma \ref{L:H-neighbors}(1), $|N_{H'}(v)| \ge \Delta^2/2$.
Thus, the probability that none of $v$'s queries result in a proposal of $\sigma$ is at most
\[ (1-1/(6000\phi\Delta^2))^{\Delta^2/2} \le  e^{-1/(12000\phi)} \le 1-1/(24000\phi)\ , \]
using the inequality $e^{-x} \le 1-x/2$, for $x \le 1/2$.
That is, the probability that there is a query $Q$ in which $\sigma$ is proposed to $v$ is at least $1/(24000\phi)$, under our assumption that no queries are dropped.

By Lemma \ref{L:survival}, a query survives with probability at least $c_6 \ge 1/7$, independent of the path it takes, and thus independent of the color it leads to.
%It only matters that the query $Q$ survives, which happens with probability at least $c_6$, by Lemma \ref{L:survival}, and this is independent of the color it proposes.
Thus, the probability that $\sigma$ gets proposed to $v$ (via some $u \in H'[v]$) is at least 
$c_6/(24000\phi)$.
\end{proof}

% Expected number of proposals received
\begin{lemma}
An active live node receives at most one proposal in expectation. This holds even in the setting where no queries are dropped. 
\label{L:proposal-expected}
\end{lemma}
\iffalse %% Proof moved to appendix
\begin{proof}
  Let $P_v$ be the set of nodes that are $H$-neighbors of $H'$-neighbors of $v$ but are not d2-neighbors of $v$. These are the only nodes that generate proposals for $v$ in Step 5. Each $H'$-neighbor $u$ of $v$ receives a query from $v$ with probability $1/(6000\phi)$ and sends it to a random $H$-neighbor. $u$ has at least $\Delta^2/3$ $H$-neighbors by Lemma \ref{L:H-neighbors}(2). Thus, the probability that a given node in $P_v$ receives a query involving $v$ in Step 5 is at most $1/(6000\phi) \cdot \Delta^2 \cdot 3/\Delta^2 = 3/(6000\phi)$. By Lemma \ref{L:h-degree}(2) and Obs.~\ref{O:sparse}, $P_v$ contains at most $24 e^3 \phi$ nodes. Hence, the expected number of proposals $v$ receives from Step 5 is $24 e^3\phi \cdot 3/(6000\phi) = 72e^3/6000 \le 1/4$.

We next bound the expected number of proposals due to Step 3.
The probability $p_u$ that a given $H'$-neighbor $u$ of $v$ picks a color that is not used among its $H$-neighbors, over the random color choices, is 
%\[p_u = (\Delta^2 - deg_{H}(u))/deg_{H}(u) \ge 3(1 - deg_H(u)/\Delta^2) \]
\[p_u = (\Delta^2 - deg_{H}(u))/deg_{H}(u) \ge 3(1 - deg_H(u)/\Delta^2)\ , \] 
where we used Lemma \ref{L:h-degree}(2) in the inequality.
The probability that a random query from $v$ leads to a color proposal is then 
  \[ \frac{\sum_{u \in H'[v]} p_u}{|N_{H'}(v)|} \ge \frac{1}{|N_{H'}(v)|\Delta^2} \sum_{u \in H'[v]} 3 \left(\Delta^2 - deg_H(u)\right) \ge  \frac{c_8 \phi}{|N_{H'}(v)|}\ ,  \]  
applying Lemma \ref{L:h-degree}(3). The expected number of queries sent from $v$ to $H'$-neighbors to $N_{H'}$ is $|N_{H'}(v)|/(6000\phi)$, so the expected number of proposals that $v$ receives is $c_8/6000 \le 2/3$.
\end{proof}
\fi  %% Proved moved to appendix

% Drop-rate for proposals
\begin{lemma}
  Let $v$ be an active live node.
  Conditioned on the event that a particular color is proposed to $v$, the probability that $v$ \emph{tries} the color is at least $c_9 \ge 1/6$.
%  The probability that a given color proposal is tried (in Step 6) is at least $c_9 \ge 1/6$.\ym{Not clear from the statement what given means. Write as in the prove. Conditioned on being proposed..}
\label{L:proposal-tried}
\end{lemma}
\iffalse %% moved to appendix
\begin{proof}
By Lemma \ref{L:proposal-expected}, $v$ receives at most one proposal in expectation.
By Markov's inequality, $v$ receives at most two proposals, with probability at least $1/2$. Let $m$ be a specific proposal. 
Conditioned on $m$ being proposed, $v$ receives at most three proposals, with probability at least $1/2$. Hence, with probability at least $1/6$, $m$ will be the proposal selected to be tried.
\end{proof}
\fi
%\ym{I still don't understand what the proposals refer to and also not the point of this sentence. Before we bounded the proposals that $v$ would get from Step 3 or from Step 5. How can $v$ get any other proposals at all? }\mh{Think of it this way. On average over time, one person is waiting at the coffee machine. But when you go there, there are usually two: you and one more person. Because once you condition on you being there, that changes the equation. How should I word this?}

\begin{lemma}
An active live node with leeway at least $\tau$ at the start of \alg{Reduce-Phase}($\phi,\tau$) becomes colored with probability $c_7 \tau/\phi$, for some constant $c_7 > 0$.
%where $c_7 \ge c_6c_9/288000$.
\label{L:progress}
\end{lemma}
\begin{proof}
Let $v$ denote the active live node and let $\Psi$ denote the set of $v$-good colors. By the leeway bound, $|\Psi| \ge \tau$. 
Let $\Phi$ be the multiset of colors proposed to (active) live d2-neighbors of $v$.
There are at most $\phi$ live d2-neighbors, and the expected fraction of them that are active is $\tau/(8\phi)$. Each active live node receives expected at most 1 proposals. Hence, the expected size of $\Phi$ is at most $\phi \cdot \tau/(8\phi) \cdot 1 = \tau/8$.
Let $A$ be the event that $\Phi$ is of size at most $\tau/4$. By Markov's inequality, $A$ holds with probability at least $\Pr[A] \ge 1-1/2 = 1/2$.

For a color $\sigma \in \Psi$, let $p_\sigma$ be the probability that $\sigma$ is proposed to some active live d2-neighbor of $v$. This dominates the probability that a d2-neighbor of $v$ actually \emph{tries} $\sigma$. 
Let $\Psi' = \{\sigma \in \Psi : p_{\sigma} \ge 1/2\}$. 
Note that $\sum_{\sigma \in \Psi} p_{\sigma} \le |\Phi| \le \tau/4$, assuming $A$ holds. 
On the other hand, the sum is at least $\sum_{\sigma \in \Psi'} p_{z} \ge \sum_{\sigma \in \Psi'} 1/2 = |\Psi'|/2$. Thus, $|\Psi'|/2 \le \tau/4$, or $|\Psi'| \le \tau/2$, assuming $A$.

Let $\hat{\Psi} = \Psi \setminus \Psi'$ and let $\sigma \in \hat{\Psi}$. Let $B_\sigma$ be the event that $v$ tries $\sigma$ while no d2-neighbor of $v$ receives a proposal of $\sigma$. Observe that the events for different $\sigma$ are independent.
By Lemma \ref{L:sigma} that $\sigma$ is proposed to $v$ is at least $c_6/(24000\phi)$ and by Lemma \ref{L:proposal-tried}, the probability that it gets tried is at least $c_9 \ge 1/6$. 
Assuming $A$ holds, $|\hat{\Psi}| \ge \tau/2$.
For $\sigma \in \hat{\Psi}$, the probability that no d2-neighbor of $v$ receives a proposal of $\sigma$ is at least $1/2$, assuming $A$.
Thus, for $\sigma \in \hat{\Psi}$,
\[ \Pr[B_\sigma] \ge \frac{c_6}{24000\phi} \cdot \frac{1}{6} \cdot \frac{1}{2} \ge \frac{1}{300000 \phi}\ , \]
Since $v$ tries only one color, the events $B_\sigma$ are disjoint.
Let $B = \cap_{\sigma \in \hat{\Psi}} B_\sigma$.
Then, $\Pr[B] = |\hat{\Psi}|/(300000\phi)$. 
Assuming $A$, $\Pr[B|A] \ge (\tau/2)(1/300000\phi) = \tau/(600000\phi)$.
Now, if $B$ holds, then $v$ becomes colored.
This happens with probability at least
$\Pr[B] \ge \Pr[B \cap A] = \Pr[A] \Pr[B|A] = 1/2 \cdot \tau/(600000\phi) = \tau/(1200000\phi)$.
\end{proof}

Since the algorithm performs $O(\log n)$ phases, and each node of leeway at least $\tau$ is colored in each phase with a constant probability, the algorithm either properly colors the node or decreases its leeway below $\tau$.
%The correctness of the algorithm now follows from Lemma \ref{L:progress} by concentration. \ym{I would rather refer to the $\rho=O(\log n)$ phases each with a constant probability to get colored or reduce the leeway than 'concentration'}

\begin{theorem}
%There is a choice of $c_2>1$ such that if all messages of one execution of \alg{Reduce} are delivered the postcondition holds, w.h.p. Namely, all nodes of leeway at least $\tau$ are colored in the call to \alg{Reduce}($\phi,\tau$), w.h.p.
All live nodes are of leeway less than $\tau$ after the call to \alg{Reduce}($\phi,\tau$), w.h.p.
%All nodes of leeway at least $\tau$ are colored in the call to \alg{Reduce}($\phi,\tau$), w.h.p.
%All nodes of leeway at least $\tau$ are colored in the call to \alg{Reduce}($\phi,\tau$), w.h.p.
\label{T:reduce-correct}
\end{theorem}

\begin{proof}
Set $c_3 = 32/c_7$. 
Let $v$ be a live node of leeway at least $\tau$ at the start of $\alg{Reduce}$.
With probability $\tau/(8\phi)$, $v$ is active in a given phase, and with
probability at least $c_7 \tau/\phi$, $v$ becomes colored in a given phase where it is active, by Lemma \ref{L:progress}. Thus, the probability that it remains live after all $\rho = c_3 (\phi/\tau)^2 \log n$ phases is at most 
$(1-c_7\cdot(\tau/\phi)^2/8)^{c_3 (\phi/\tau)^2 \log n} \le e^{-4\log n} \le n^{-4}$.
The probability that some such node remains uncolored is at most $n^{-3}$.
\end{proof}


Observe that after \alg{Reduce}($\phi$,1), all nodes are colored, w.h.p., since a live node always has leeway at least 1. 
Corollary \ref{C:first-rand-result} follows.


\subsection{Algorithm with Improved Final Phase}
\label{ssec:improved}

We now give an improved algorithm for d2-coloring that uses $\Delta^2+1$ colors and runs in $O(\log \Delta\log n)$ rounds. We assume $\Delta$ is known to the nodes.
This is achieved by replacing the final phase of \alg{d2-color} (i.e., the last step) with a different approach.
%We now given an improved algorithm for the last step of \alg{d2-Color} that produces a $\Delta^2+1$-coloring runs in $O(\log n)$ rounds. Thus, we let $D=\Delta^2$ here, and assume $\Delta$ is known to the nodes. This improves the overall time complexity of the algorithm to $O(\log \Delta \log n)$.

%\mypar{Intuition}
In the final phase of the improved algorithm, the nodes cooperate to track the colors used by the d2-neighbors of each live node. Thus, they learn the \emph{remaining palette}: the set of colors of $[\Delta^2]$ not used by d2-neighbors. Gathering the information about a single live node is too much for a single node to accumulate, given the bandwidth limitation. Instead, each live node chooses a set of \emph{handlers}, each handling a subrange of its color spectrum. The colored nodes then need to forward their color to the appropriate handler of each live d2-neighbor. After learning about the colors used, each of the multiple handlers choose an unused color at random and forward it to the live node. The live node selects among the proposed colors at random and tries it (which works with constant probability).

Since no routing information is directly available, we need to be careful how the coloring information is gathered at the handlers. We use here a \emph{meet-in-the-middle} approach. Each handler informs a random subset of its d2-neighbors about itself and each colored node sends out its message along a host of short random walks. In most cases, if the numbers are chosen correctly, a random walk will find an informed node, which gets the message to the handler. 

Once the unused palette is available, the coloring can be finished up in $O(\log n)$ rounds in the same fashion as the basic randomized {\congest} algorithm for ordinary coloring.

\begin{quote}
   \textbf{Algorithm} \emph{Improved-d2-Color}
%    \emph{Precondition}: Each node has at most $\tau/\Delta$ immediate live neighbors \\
%    \emph{Postcondition}: The graph is properly $D+1$-colored \\

   If $\Delta^2 \ge c_2\log n$ then  \\
%   1. Each node picks a random color and keeps it if none of its d2-neighbors picked it. \\
   \hspace*{2em} repeat $c_0 \log n$ times: \\
%   1. repeat $\Theta(\log n)$ times: \\
   \hspace*{4em} Each live node picks a random color and \emph{tries} it. \\     
   \hspace*{2em} Form the similarity graphs $H=H_{2/3}$ and $\hat{H} = H_{5/6}$ \\
   \hspace*{2em} for ($\tau \leftarrow c_1 \Delta^2$; $\tau > c_2\log n$; $\tau \leftarrow \tau/2$) \\
   \hspace*{4em}      \alg{Reduce}($2 \tau$, $\tau$) \\
    \alg{LearnPalette}() \\
    \alg{FinishColoring()}
\end{quote}

The main effort of this section is showing how to learn the remaining palette in $O(\log n)$ steps.
%\section{Improved Final Phase}
We first show how that information makes it easy to color the remaining nodes.

\mypar{Finishing the coloring}
%
Suppose each node has $O(\log n)$ live d2-neighbors and knows the remaining palette. This includes the case when $\Delta^2 \le c_2\log n$, in which case no d2-neighbors are yet colored. 
We can then simulate the basic randomized algorithm for ordinary colorings with constant overhead, to complete the coloring in $O(\log n)$ rounds.

This algorithm, \alg{FinishColoring}, proceeds as follows:
Each node $v$ repeats the following two-round procedure until it is successfully colored. 
Flipping a random coin, $v$ is quiet or tries a random color from $T'_v$, with equal probability $1/2$.
If it succeeds, it forwards that information to immediate neighbors. They promptly forward it to each of their immediate neighbors $w$, who promptly updated their remaining palette $T'_w$. If a node has a backlog of color notifications to forward, it sends out a \textsc{Busy} message. A node with a \textsc{Busy} neighbor then waits (stays quiet) until all notifications have been forwarded (and all \textsc{Busy} signals have been lifted from its immediate neighbors). 
  %  \item $v$ tries a random color from $T'_v$ until it is successfully colored. When it becomes colored, it informs the other live d2-neighbors of its color, who update their set $T'_v$ accordingly.
  %the colors in $T'_v$ in sequence, until colored.

\begin{lemma}
\alg{FinishColoring} completes in $O(\log n)$ rounds, w.h.p.
\label{l:step7}
\end{lemma}

\begin{proof}
A node waits for a busy neighbor for at most $O(\log n)$ rounds, since it has that many live d2-neighbors.
Consider then a non-waiting round.
With probability 1/2, at least half of the live d2-neighbors of $v$ are quiet. In this case, at least half of the colors of $v$'s palette are not tried by d2-neighbors, and hence, $v$ succeeds with probability at least $1/2$. The expected number of rounds is therefore $O(\log n)$, and by Chernoff (\ref{eq:concentration}), this holds also w.h.p.
\end{proof}

\mypar{Learning the Available Palette}
%
Let $\varphi \le c\log n$ be an upper-bound on the leeway of live nodes.
Let $Z$ and $P$ be quantities to be determined.
We call each set $B_i = \{i\cdot \Delta^2/Z, i\cdot \Delta^2/Z+1, i\cdot \Delta^2/Z+2, \ldots, (i+1)\Delta^2/Z-1 \}$, $i=0,1,\ldots, Z-1$, a \emph{block} of colors. The last block $B_{Z-1}$ additionally contains the last color, $\Delta^2$. 
There are then $Z$ blocks % $B_1,\ldots,B_Z$ 
that partition the whole color space $[\Delta^2]$.

\medskip

%\begin{quote}
   \textbf{Algorithm} \emph{LearnPalette}()
   
    \emph{Precondition}: Live nodes have leeway at most $\varphi \le c_2\log n$.
%    , and each node has at most $\varphi/\Delta$ live immediate neighbors. \\

  \emph{Postcondition}: Live nodes know their remaining palette
%  \emph{Postcondition}: The graph is properly $d2$-colored.

\begin{enumerate}
  \item If $\Delta = O(\log n)$, then the nodes learn the remaining palette in $\Delta$ rounds by flooding, and halt.
  \item Each node learns of its live d2-neighbors by flooding.
  \item For each live node $v$ and each block $i \in \{1,\ldots, \Delta\}$ of colors, a random $H$-neighbor $z^i_v$ of $v$ is chosen.
  % We verify that $v$ and $z^i_v$ share at least 1/2 of their d2-neighbors, by comparing $O(\log n)$-sized random samples of their neighborhoods, reselecting $z^i_v$ if needed.
  \item Each node $z^i_v$ picks a random subset $Z_v^i$ of $P$ d2-neighbors (formed as a set of random 2-hop paths).
  %\footnote{Do we need to constrain these nodes to be d2-neighbors of $v$?}
  It informs them that it "handles" block $i$ of the palette of live node $v$ (which indirectly tells them also the 2-path back to $z^i_v$). 
  \item Each colored node $u$ with color $c_u$ attempts to forward its color to some node in $Z_v^i$, where $i = \lfloor c(u) / \Delta \rfloor$, for each live d2-neighbor $v$. 
  This is done by sending the color along $\Theta(\Delta^2/P \cdot \log n)$ different random 2-paths. The node in $Z_v^i$ then forwards it directly to $z^i_v$. Let $C^i_v$ denote the set of colors that $z^i_v$ learns of.
  \item Each node $z_v^i$ informs $v$ by pipelining of the set $T_v^i = B_i \setminus C_v^i$ of colors missing within its range. 
  \item $v$ informs its immediate neighbors by pipelining of $T_v= \cup_i T_v^i$, the colors that it has not learned of being in its neighborhood. Each such node $w$ returns the set $\hat{T}_{v,w}$, consisting of the colors in $T_v$ used among $w$'s immediate neighbors. $v$ removes those colors from $T_v$ to produce $T'_v = T_v \setminus \cup_w \hat{T}_{v,w}$, which yields the true remaining palette $[\Delta^2] \setminus T'_{v}$.
%  \item $v$ informs all its d2-neighbors of $T_v= \cup_i T_v^i$ (by pipelining).
%  \item Each d2-neighbor of $v$ whose color appears in $T_v$ informs $v$ of its color. Those colors are removed from $T_v$ to produce $T'_v \subseteq T_v$.
\end{enumerate}
%\end{quote}
%  Observe that $T'_v$ maintains the current set of available colors for $v$, after Step 6 and through each iteration of Step 7. 
 \medskip
 
We first detail how a node $u$ selects a set of $m$ random d2-neighbors, as done in Steps 3, 4, and 5. It picks $m$ edges (with replacement) to its immediate neighbors at random and informs each node $w$ of the number $m_w$ of paths it is involved in. Each immediate neighbor $w$ then picks $m_w$ immediate neighbors. This way, $u$ does not directly learn the identity of the d2-neighbors it selects, but knows how to forward messages to each of them. Broadcasting or converge-casting individual messages then takes time $\max_{w \in N_G(v)} m_w$, which is $O(m/\Delta + \log n)$, w.h.p. (by (\ref{eq:concentration})).

The key property of this phase is the following.

\begin{lemma}
$|T_v| = O(\log n)$, for every live node $v$, w.h.p.
%If a live node $v$ is not $\zeta$-sparse, then all but $O(\zeta)$ colors of $v$'s d2-neighbors get recorded in $C_v = \cup_i C^i_v$, w.h.p.
\label{l:last}
\end{lemma}

\begin{proof}
By assumption, live node $v$ has leeway $O(\log n)$ at the start of the algorithm, and thus it has slack $O(\log n)$. 
By the contrapositive of Prop.~\ref{P:sparsity}, it is $\zeta$-sparse, for $\zeta=O(\log n)$. 
Thus, the $H$-degree of $v$ is at least $\Delta^2 - 40\zeta$, by Lemma~\ref{L:h-degree}(1). For all $H$-neighbors of $v$, a random 2-hop walk has probability at least $|Z_v^i|/\Delta^2 = P/\Delta^2$ of landing in $Z_v^i$. 
Thus, w.h.p., one of the $\Theta((\Delta^2/P)\log n)$ random walks ends there, resulting in the color being recorded in $C_v$. 
Hence, w.h.p., $|T_v| \le |N_{G^2}(v)\setminus N_H(v)| \le 40\zeta = O(\log n)$.
\end{proof}

% The correctness is immediate from the statement of the algorithm. 
%We now argue the round complexity of the algorithm.
A careful accounting of the time spent yields that the dominant terms of the complexity are: $O(\log n)$ (Steps 2, 6-7), $O(PZ\varphi/\Delta^3) = O(PZ (\log n)/\Delta^3)$ (Step 4), $O(\Delta\varphi/P \log n) = O(\Delta(\log n)^2/P)$ (first half of Step 5), and $O(\Delta/Z \log n)$ (second half of Step 5). To optimize, we set $Z = \Delta$ and $P = \Delta \sqrt{\Delta\log n}$, for time complexity of $O(\log n (1 + \sqrt{(\log n)/\Delta}))$, which is $O(\log n)$ when $\Delta = \Omega(\log n)$. 

\begin{theorem}
%The time complexity of \alg{AccountFor}($\varphi$) is $O(\varphi^{2/3}/\Delta^{1/3} \cdot \log n)$.
The time complexity of \alg{LearnPalette}($\varphi$) with $\varphi=O(\log n)$ is $O(\log n)$, when $\Delta = \Omega(\log n)$.
\label{T:learnpalette}
\end{theorem}
\iffalse %%% Moved to appendix
\begin{proof}
Each live node has $\Omega(\Delta^2)$ neighbors and selects $Z$ of them uniformly to become handling nodes. Thus, each node has probability $O(Z/\Delta^2)$ of becoming a handling node for a given live d2-neighbor, and since it has $O(\varphi)$ live d2-neighbors (by the precondition), it becomes a handling node for an expected $O(\varphi \cdot Z/\Delta^2)$ live nodes. 
%\footnote{Here we use a weaker form of the precondition}
\begin{itemize}
\item The flooding in Step 2 takes as many rounds as there are live nodes in a immediate neighborhood, which is $O(\varphi)$.
\item In Step 3 involves sending a single message to each handling node (of each live node), which is easily done in $O(1)$ expected time, or $O(\log n)$ time, w.h.p.
\item In Step 4, each node forwards expected $P/\Delta$ messages from each handling immediate neighbor, and it has $O(Z\varphi/\Delta)$ such immediate neighbors. 
Thus, it sends out $O(P Z\varphi/\Delta^2)$ messages to random neighbors, or $O(P Z \varphi/\Delta^3)$ message per outgoing edge, w.h.p. This takes time $O(PZ \varphi/\Delta^3)$. 
\item In Step 5, a colored node needs to forward its color to the handling node $z_v^i$ of each of its $O(\varphi)$ live d2-neighbors. Since it is sent along $\Delta^2/P \log n$ paths, and due to the conductance,  w.h.p., the color reaches a node in $Z_v^i$ (the set of nodes informed of $z_v^i$). 

The path from a colored node $u$ to a handler $z_v^i$ for a live node has two parts: the path $p_{u,w}$ from $u$ to an node $w$ that knows the path to $z_v^i$, and path $q_{w,z_v^i}$ from $w$ to $z_v^i$. 
A given node $a$ has probability $1/\Delta^2$ of being an endpoint of a given path $p_{u,w}$ (from a d2-neighbor); there are $O(\varphi)$ live d2-neighbors (by the precondition, weaker form), $\Delta^2$ colored d2-neighbors, and $\Delta^2/P \log n$ copies of messages sent about each. Thus, a given node has expected 
\[ O(\varphi \cdot \Delta^2 \cdot \Delta^2/P \log n \cdot 1/\Delta^2) = O(\varphi/P \cdot \Delta^2 \log n) \] 
random paths going to it.
Similar argument holds for a node being an intermediate node on a path $p_{u,w}$: the number of immediate neighbors goes to $\Delta$, while the probability of being a middle point on the path goes to $1/\Delta$, resulting in the same bound.
Thus, the load on each edge is $O(\Delta \varphi/P \log n)$. 

For the $q$-paths, the main congestion is going into the handler. Observe that there are only $O(\log n)$ $p$-paths that reach an informed node $w$. Hence, the number of paths going into a given handler is the product of the size of the block, times $\log n$: $\Delta^2/Z \cdot \log n)$. So, the load on an incoming edge into a handler is $O(\Delta/Z \log n)$, w.h.p. (when $Z = O(\Delta)$).

\item The pipelining of Steps 6-7 takes $O(|T_v|)$ rounds, and by Lemma \ref{l:last}, $|T_v| = O(\log n)$, w.h.p. 

\end{itemize}

To summarize, the dominant terms of the time complexity are $O(\log n)$ (Steps 2, 6-7), $O(PZ\varphi/\Delta^3) = O(PZ (\log n)/\Delta^3)$ (Step 4), $O(\Delta\varphi/P \log n) = O(\Delta(\log n)^2/P)$ (first half of Step 5), and $O(\Delta/Z \log n)$ (second half of Step 5).

Optimizing, we set $Z = \Delta$ and $P = \Delta \sqrt{\Delta\log n}$, 
for time complexity of $O(\log n (1 + \sqrt{(\log n)/\Delta}))$, which is $O(\log n)$ when $\Delta = \Omega(\log n)$. 
\end{proof}
\fi %% moved to appendix



Combining Thm.~\ref{T:learnpalette} and Lemma \ref{l:step7} with
Thm.~\ref{T:reduce-correct} of the previous subsection,
we obtain our main result.
%we obtain the following bound on \alg{Improved-d2-Color}.

\smallskip
\textsc{\Cref{thm:d2ColoringRand}.} 
\emph{There is a randomized \CONGEST algorithm that d2-colors a graph with $\Delta^2+1$ colors in $O(\log \Delta \log n)$ rounds, with high probability.}
